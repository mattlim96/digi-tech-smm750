{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Preprocessing\n",
    "\n",
    "## 1. Data cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First need to import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"chimera_data_not_cleaned.csv\"\n",
    "df = pd.read_csv(path, sep = \",\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a histogram of the `age` column of the dataset. Do the values look reasonable to you? Do the same for `salary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[\"age\"],kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[\"salary\"],kde=False,bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function `.unique()` to take a look at the values taken on by `education`, `years_since_promotion`, and `exit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"education\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"years_since_promotion\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['exit'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function `.duplicated()` and conditioning to to detect if there are any duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dups = df.duplicated() #checks each row of the dataset and returns TRUE or FALSE depending on whether it is a duplicate\n",
    "print(dups.any()) #returns TRUE if there is any value in dups that is equal to TRUE\n",
    "print(df[dups]) #returns the problematic row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function `.drop_duplicates(inplace=True)` to obtain a new dataset with no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape) #gives current size of dataset\n",
    "df.drop_duplicates(inplace=True) # delete duplicate rows\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function `.isna()` and conditioning to detect if there are any empty cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to drop entire rows with NA values, we can simply use\n",
    "`df.dropna(axis = 0,how=\"any\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis = 0,how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function `.nunique()` to find the number of unique entries for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop any columns with only one value using `.drop(columns=['name1','name2'])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['local'])\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scaling/Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 1., -1.,  2.],\n",
    "                [ 2.,  0.,  0.],\n",
    "                [ 0.,  1., -1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of normalizing (putting the data in the [0,1] range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_minmax = min_max_scaler.fit_transform(X)\n",
    "X_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of scaling (making sure that the data has mean 0 and variance 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = preprocessing.scale(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data imputation \n",
    "\n",
    "This can be done in Python using the following library. See the documentation at https://scikit-learn.org/stable/modules/impute.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import impute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can replace missing values by the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 1., np.nan,  2.],\n",
    "                [ 2.,  0.,  np.nan],\n",
    "                [ 0.,  1., -1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = impute.SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(X)\n",
    "print(imp.transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = impute.SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(df)\n",
    "dfnew = pd.DataFrame(imp.transform(df))\n",
    "dfnew.columns = df.columns\n",
    "print(dfnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see why this could be a problem? Try out the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew['exit'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will simply remove all rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.dropna(axis=0,inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `seaborn`, plot a boxplot of `salary` as `exit` varies. Are there any outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x=\"exit\",y=\"salary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function `stats.zscore(df[Column])` compute the z-score table for `salary`. Are there any outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "Z = stats.zscore(df[\"salary\"],nan_policy=\"omit\") #compute z-score table\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What data structure is obtained here? Find the index of the outlier in this case using `np.where` and logical conditions. Do the two indexes correspond?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where((Z>3) | (Z<-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at `boss_survey` as `exit` varies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x=\"exit\",y=\"boss_survey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function `np.nanquantile(column,quantile)` to find the 5% quantile of `boss_survey` results within the employees exiting the firm. Then, take a look at all of the employees leaving the firm who have a `boss_survey` result at or below this 5% quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exiteers=df[df[\"exit\"]==1]\n",
    "lower_quantile_survey = np.nanquantile(df_exiteers[\"boss_survey\"],0.05)\n",
    "df_exiteers[df_exiteers[\"boss_survey\"] <= lower_quantile_survey]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Numerical to Categorical\n",
    "We start with ordinal then move onto one-hot encoding.\n",
    "\n",
    "1. Ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "dftest = pd.DataFrame({'size':['small','medium','large','small','large','medium']})\n",
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder(categories=[['small','medium','large']]) \n",
    "dfnew = encoder.fit_transform(dftest) # transform data\n",
    "pd.DataFrame(data=dfnew, columns=dftest.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = pd.DataFrame({'color':['green','red','red','blue','green','red']})\n",
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(dftest,drop_first=True,columns=['color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.apply(np.log)` to transform the column `salary` of `dfcopy` from itself to the log of itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy['salary'] = dfcopy['salary'].apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print both df and dfcopy. Check their `salary` column. Has your transform had the effect you wanted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make a graphical comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[\"salary\"],kde=False,bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(dfcopy[\"salary\"],kde=False,bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new feature in the dfcopy dataset consisting of the `boss_tenure_percentage`=`boss_tenure` / `tenure`. Then, print a histogram, using `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy[\"boss_tenure_percentage\"]=dfcopy[\"boss_tenure\"]/dfcopy[\"tenure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(dfcopy[\"boss_tenure_percentage\"],kde=False,bins=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Exercises\n",
    "\n",
    "## Exercise 1: Iceberg right ahead!\n",
    "\n",
    "Next term, we will use the Titanic dataset, available at https://www.kaggle.com/c/titanic/data \n",
    "This is historic data containing the passengers present on the Titanic and some of their features (whether, e.g., they had family on board or not, their cabin numbers, etc.) and whether or not they survived the boat sinking.\n",
    "Our goal is to clean up this dataset in view of using it later down the line. The dataset we will be cleaning up is `titanic_train.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train=pd.read_csv(\"titanic_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Observe the header of the dataset. What do SibSp and Parch represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Are there any duplicates in the dataset? Why are we doing this before dropping any columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Drop a couple of columns from the datasets: PasssengerID, Name, and Ticket Number. We drop Ticket Number and PassengerId as they don't have much informative value. The Name could have some information in it (e.g., nobility, married or not, etc.) but that would require natural language processing, which we will not use on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train = titanic_train.drop(columns=[\"PassengerId\",\"Name\",\"Ticket\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Let's check for inconsistencies in the numerical data using `.hist()`. Does anything seem abnormal to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some outliers but it seems like the values obtained are coherent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What values do the categorical variables (this includes the cabin number) take on? Is there anything irregular there? Make sure you understand their output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train[\"Embarked\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corresponds to the port at which the passenger embarked (e.g. S is for Southampton). There seems to be one or more missing values here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train[\"Cabin\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corresponds to the cabin booked by the passenger. Some passengers seem to have booked many cabins at the same time. We check that this is indeed the case by taking a look at the fare paid for e.g. `B57 B59 B63 B66`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train[titanic_train[\"Cabin\"]==\"B57 B59 B63 B66\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fare is considerably more expensive so it tends to show that indeed, it was possible to book many cabins at once. There also seems to be one or more missing values here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train[\"Sex\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be no issue with this column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. We now deal with the missing values. Which features are missing information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be an awful lot of cabins missing, as well as age, and embarcation port."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Which percentage of Age/Cabin/Embarked are missing? Use `.shape` to find this. In consequence, what should you with the Cabin column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "177/891"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "687/891"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "77% of the data in Cabin data is missing: this is a huge amount and probably not a very good predictor. We remove this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train = titanic_train.drop(columns=[\"Cabin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. For the Age column, we use a nearest neighbor approach. Use `KNNImputer` to fill in the missing values. Check that there are no more missing values in the Age column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=1)\n",
    "titanic_train[[\"Age\"]] = imputer.fit_transform(titanic_train[[\"Age\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. For the Embarked column, use `countplot` in the `seaborn` package to obtain the number of people who embarked at `S`, `C`, and `Q`. Where did the overwhelming majority of passengers embark? Use `SimpleImputer` to simply replace all missing values with the most frequent one. Check that no more entries are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Embarked\", data=titanic_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp=SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
    "titanic_train[[\"Embarked\"]]=imp.fit_transform(titanic_train[[\"Embarked\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. The division into Parch and SibSp is quite random. We regroup this column into one column called `Family_Presence`. Create a new column in the dataframe called `Family_Presence` which contains 1 if either SibSp is equal to 1 or Parch is equal to 1. Then drop `Parch` and `SibSp`. Hint: Use `np.where(condition,1,0)` where `condition` is the logical condition needed to be satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train[\"Family_Presence\"]=np.where((titanic_train[\"SibSp\"]>=1) | (titanic_train[\"Parch\"]>=1), 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train = titanic_train.drop(columns=[\"SibSp\",\"Parch\"])\n",
    "titanic_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Finally, replace all categorical variables by numerical ones. We are ready to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train = pd.get_dummies(titanic_train,drop_first=True)\n",
    "titanic_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "\n",
    "Recall the notion of scaling and consider a feature for which we have many observations.\n",
    "\n",
    "1. Show that if we take the feature vector, subtract its mean and divide by its standard deviation, then the new feature vector obtained is scaled, i.e., has mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We subtract the mean $\\mu$ of the feature vector from $x_i$ and divide by its standard deviation $\\sigma$. In this way, each feature has now become $\\frac{x_i-\\mu}{\\sigma}$. \n",
    "So, the average of this new feature vector is:\n",
    "$$\\frac{\\frac{x_1-\\mu}{\\sigma}+\\ldots+\\frac{x_n-\\mu}{\\sigma}}{n}=\\frac{x_1+\\ldots+x_n-n \\cdot \\mu}{n \\sigma}=\\frac{1}{\\sigma}\\cdot \\left( \\frac{x_1+\\ldots+x_n}{n} -\\mu\\right)=\\frac{1}{\\sigma}\\cdot \\left( \\mu -\\mu\\right)=0.$$\n",
    "\n",
    "Recall that the variance of numbers $y_1,\\ldots,y_n$ is $$\\frac{(y_1-\\bar{y})^2+\\ldots+(y_n-\\bar{y})^2}{n}$$ where $\\bar{y}$ is the average or mean of the numbers $y_1,\\ldots,y_n$.\n",
    "Here, we have just shown that the mean of the new feature vector is 0. Hence its variance is given by:\n",
    "$$\\frac{(\\frac{x_1-\\mu}{\\sigma})^2+\\ldots+(\\frac{x_n-\\mu}{\\sigma})^2}{n}=\\frac{1}{\\sigma^2} \\cdot \\frac{(x_1-\\mu)^2+\\ldots+(x_n-\\mu)^2}{n}=\\frac{1}{\\sigma^2} var(x_1,...,x_n)=\\frac{1}{\\sigma^2} \\cdot \\sigma^2=1.$$\n",
    "As the standard deviation is the square root of the variance, it follows that the new feature vector has standard deviation one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Check your answer on the first column of the np.array X below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 1., -1.,  2.],\n",
    "              [ 2.,  0.,  0.],\n",
    "              [ 0.,  1., -1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X[:,0]-X[:,0].mean())/(X[:,0].std())\n",
    "# we get the same thing as when we use the preprocessing library of sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Motorcycle Helmets with Bluetooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the exercise description within Moodle!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We first create the two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_demand={'Price':[91,74.63,65.06,58.27,53.00],'Demand':[1000,2000,3000,4000,5000]}\n",
    "df_demand=pd.DataFrame(data=dict_demand)\n",
    "df_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_supply={'Price':[9,23.21,31.53,37.42,42],'Supply':[1000,2000,3000,4000,5000]}\n",
    "df_supply=pd.DataFrame(data=dict_supply)\n",
    "df_supply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We now plot both dataframes on the same graph using seaborn. We want to plot a scatterplot so we use `sns.lineplot`. The curves do not intersect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"Price\", y=\"Demand\", data=df_demand,color=\"red\");\n",
    "plot=sns.lineplot(x=\"Price\", y=\"Supply\", data=df_supply,color=\"blue\");\n",
    "plot.set_ylabel(\"Supply/Demand\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We add a new column to each dataframe by taking the log-transform of the supply/demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_supply[\"Log_supply\"]=df_supply[\"Supply\"].apply(log)\n",
    "df_supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand[\"Log_demand\"]=df_demand[\"Demand\"].apply(log)\n",
    "df_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"Price\", y=\"Log_demand\", data=df_demand,color=\"red\")\n",
    "sns.lineplot(x=\"Price\", y=\"Log_supply\", data=df_supply,color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get two lines.\n",
    "\n",
    "4. We can estimate their slopes and intercepts quite easily: slopes=rise/run and intercept=y-axis - slope * x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slope of demand:\n",
    "a=(df_demand[\"Log_demand\"].loc[4]-df_demand[\"Log_demand\"].loc[0])/(df_demand[\"Price\"].loc[4]-df_demand[\"Price\"].loc[0])\n",
    "print(a)\n",
    "#intercept of demand\n",
    "b=df_demand[\"Log_demand\"].loc[4]-a*df_demand[\"Price\"].loc[4]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slope of supply\n",
    "c=(df_supply[\"Log_supply\"].loc[4]-df_supply[\"Log_supply\"].loc[0])/(df_supply[\"Price\"].loc[4]-df_supply[\"Price\"].loc[0])\n",
    "print(c)\n",
    "#intercept of demand\n",
    "d=df_supply[\"Log_supply\"].loc[4]-c*df_supply[\"Price\"].loc[4]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We can now solve for when they cross by setting $ap+b=cp+d$, i.e., $(a-c)p=d-b$ and so $p=(d-b)/(a-c)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=(d-b)/(a-c)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. a. We construct two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1=np.arange(0,31)/10\n",
    "L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2=np.exp(L1)\n",
    "L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(L1,L2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.b. The supply curve looks like an exponential function. The demand curve looks like a decreasing exponential function.\n",
    "\n",
    "6.c. If $D=m\\cdot e^{np}$ then taking the logarithm on both sides, \n",
    "$$\\log(D)=\\log(m\\cdot e^{np})=\\log(m)+\\log(e^{np})=\\log(m)+np$$ using basic rules of the logarithm and the fact that the exponential and logarithm are inverses of one another. Hence, $\\log(D)$ is a linear function of the price $p$ and we have $a=n$ and $\\log(m)=b$. A similar reasoning can be applied to $S$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtvc_env",
   "language": "python",
   "name": "dtvc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
