{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz as gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this tutorial, we are considering the *Retention modeling at Scholastic Travel Company* case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Pre-processing the data\n",
    "\n",
    "The goal of this Break Out is to brainstorm some ideas for data preprocessing/feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stca_raw = pd.read_csv(\"stca_raw_data.csv\")\n",
    "stca_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this space to investigate the data if need be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Predicting returning customers using logistic regression\n",
    "\n",
    "We use here a preprocessed and engineered dataset. Of course you are more than welcome to use your own cleaned dataset if you would like to. The goal now is to proceed with classification and get some practice using the methods seen in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stca = pd.read_csv(\"stca_clean.csv\")\n",
    "stca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the labels `y` and the feature matrix `X` as discussed in class. Recall that we are trying to predict the outcome `\"Retained.in.2012.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the data into training/validation/testing with percentages 60/20/20, using `train_test_split`. Why are we creating a validation set here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `scikit` run a logistic regression on `X_train,y_train` with the parameter `max_iter` set to 2000 (that is, use `LogisticRegression(max_iter=2000)`. What are the 5 largest coefficients and the 5 smallest? Do they make sense intuitively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below prints out the five largest coefficients (assuming your training set is called `X_train` and your logistic regression model is called `classifier_LR`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame([X_train.columns,classifier_LR.coef_[0]]).T.sort_values(by = 1, ascending = False)\n",
    "summary.columns = ['Variable','Coefficient']\n",
    "summary.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next piece of code prints out the five smallest coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.tail(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the predicted probabilities for `X_validation`, using `[model-name].predict_proba(X_validation)[:,1]`. What are different ways of measuring how good the method is? We'll take a look at the ROC curve (using `metrics.roc_curve`), then use the area under the curve here given by `metrics.roc_auc_score` to evaluate the quality of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the area under the curve is generated with different values of the threshold. Our goal is now to think about a threshold. What is a false positive here / a false negative? Which one do you think we should focus on assuming that we adapt our marketing policy based on the output of our algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the threshold to 0.7 using `np.where`. Then, obtain the `metrics.confusion_matrix`, as well as the `metrics.accuracy_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Setting a threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good starting point for setting a threshold is the population average (0.613 in our case). There are other thresholds to set, that try to balance the true positive and false positive rates in an effective way. One example is [Youden’s J statistic](https://en.wikipedia.org/wiki/Youden%27s_J_statistic). This is simply calculated as:\n",
    "\n",
    "$J = Sensitivity + Specificity – 1  = True Positive Rate – False Positive Rate$\n",
    "\n",
    "We have this available directly from creating the ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = tpr - fpr\n",
    "J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach involves maximizing $J$. Hence, we simply pick the threshold with the highest $J$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best threshold according to the J statistic is \" + str(thresholds[np.argmax(J)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the confusion matrix at this threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = thresholds[np.argmax(J)]\n",
    "y_validation_pred = np.where(y_pred_prob < threshold, 0, 1)\n",
    "metrics.confusion_matrix(y_validation, y_validation_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other similar metrics, of course. But of course, these are not dependent on the specifc costs of false positives and false negatives (recall that in some applications, FP are more expensive, and in others, FN are more expensive).\n",
    "\n",
    "If we know the cost of any of the outcomes, we can directly compute the cost of our prediction mistakes. To go back to the example above, let's make a few assumptions:\n",
    "\n",
    "- STC only markets to groups it thinks will not be retained, at a cost of £100 per group\n",
    "- A non-retained group that receives marketing will be convinced otherwise\n",
    "- Any group going on a trip (whether retained, or because it receives marketing), brings in a benefit of £1,000\n",
    "\n",
    "What does this mean for STC's profits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. True negative: we market to this group, and it would in fact not have been retained otherwise. The net-profit then is £900\n",
    "2. False negative: we market to this group, even though it would have been retained. The net-profit for such a group is £900\n",
    "3. False positive: we assume the group is not retained, so we don't market to it (and lose it). The net-profit here is £0\n",
    "4. True positive: we correcly assume that the group is retained, and we don't market to it. The net-profit here is £1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this, we can now calculate the profits we would get from the validation customers, using the confusion matrix (we start again with a threshold of 0.7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "y_validation_pred = np.where(y_pred_prob < threshold, 0, 1)\n",
    "cm = metrics.confusion_matrix(y_validation, y_validation_pred)\n",
    "profit = cm[0][0] * 900 + cm[1][0] * 900 + cm[0][1] * 0 + cm[1][1] * 1000\n",
    "print(\"The profit at threshold \" + str(threshold) + \" is \" + str(profit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try for the threshold given by the J-statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = thresholds[np.argmax(J)]\n",
    "y_validation_pred = np.where(y_pred_prob < threshold, 0, 1)\n",
    "cm = metrics.confusion_matrix(y_validation, y_validation_pred)\n",
    "profit = cm[0][0] * 900 + cm[1][0] * 900 + cm[0][1] * 0 + cm[1][1] * 1000\n",
    "print(\"The profit at threshold \" + str(threshold) + \" is \" + str(profit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how to optimize this, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: CART for classification\n",
    "\n",
    "We now move onto using CART for classification. We will also be using the Area Under the Curve (AUC) to measure how good our model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by fitting a Classification Tree to the data with `max_leaf_nodes=8`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_DT = DecisionTreeClassifier(max_leaf_nodes = 8)\n",
    "classifier_DT.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the code below to plot the tree. Which variables seem to intervene? Are they similar to the ones obtained for Logistic Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "dot_data = export_graphviz(classifier_DT, feature_names = X_train.columns, filled = True, rounded = True, class_names=[\"Not Retained\",\"Retained\"])\n",
    "graph = gp.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use it to obtain the predicted probabilities of retention (i.e., classifier = 1) on `X_validation` using `.predict_proba`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = classifier_DT.predict_proba(X_validation)[:,1] # probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is the model? Compute the AUC for this model and the accuracy using the same threshold as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y_validation, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validation_pred = np.where(y_pred_prob > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_validation,y_validation_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_validation, y_validation_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were advising this company, what model would you recommend they use? Retrain the model you have selected on the training+validation set, then test it on the test set and report the AUC and profit on the test set. (These could be useful for the company to have.) Comment on the robustness of the approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would there be any other recommendations except for the model and the consequent predictions that you would give to the company based on your analyses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
