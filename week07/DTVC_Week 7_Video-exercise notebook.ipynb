{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Linear regression in Python\n",
    "\n",
    "For this first part, we will be using a very small and easy dataset for illustrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm #to import r dataset\n",
    "df=sm.datasets.get_rdataset(\"women\").data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains weights and heights. As the following plot shows, these naturally follow a nice relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[[\"height\"]],df[[\"weight\"]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with scikit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can measure this relationship with linear regression. So far, we have looked at `statsmodels`. We now also explore `sklearn` (scikit-learn), which has more of a machine learning-orientation (it allows us to easily make and measure predictions). The key benefit of `sklearn` is that other machine learning tools can be run with pretty much the same commands - so once you understand one algorithm, it is easy to switch to more complex ones.\n",
    "\n",
    "Note: there is no easy way to obtain other model outputs (p-values, etc.) from sklearn, as these outputs are not present in other, non-regression, machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[[\"height\"]]\n",
    "Y=df[[\"weight\"]]\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, Y) # Fit a linear regression with vector Y as dependent and matrix X as independent\n",
    "\n",
    "print(\"Intercept = \",lm.intercept_) # Print the resultant model intercept \n",
    "\n",
    "print(\"Model coefficients = \", lm.coef_) # Print the resultant model coefficients (in order of variables in X)\n",
    "\n",
    "print(\"R^2 =\",lm.score(X,Y)) # Print the resultant model R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the result using scikit's predict function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=lm.predict(X)\n",
    "plt.scatter(X,Y)\n",
    "plt.plot(X,Y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Polynomial regression in Python\n",
    "\n",
    "We present both scikit-learn and statsmodels methods here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Polynomial regression with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polynomial regression is, in fact, a linear regression on a transformed set of X-variables. We have to define the degree of the polynomial, however:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "degree=3\n",
    "X=df[[\"height\"]]\n",
    "Y=df[[\"weight\"]]\n",
    "\n",
    "poly = PolynomialFeatures(degree) #define a polynomial\n",
    "X_poly=poly.fit_transform(X) #map all the values of X as [1,x,x^2,x^3, etc]\n",
    "X_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyreg = LinearRegression().fit(X_poly, Y) # try and find coefficients c1+c2*x+x3*x^2+... via linear regression\n",
    "\n",
    "print(polyreg.coef_) #print these coefficients\n",
    "print(polyreg.score(X_poly,Y)) #print R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we plot the curve: We see that the fit with a polynomial seems much better than with a linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=polyreg.predict(X_poly) #the predicted points y from our model on X_poly, which is the transform of X.\n",
    "plt.scatter(X,Y)\n",
    "plt.plot(X,y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Polynomial regression with statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process in `statsmodels` is similar, but requires the use of the `formula.api`. A benefit, however, is that we can print out a summary, including p-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "X=df[\"height\"]\n",
    "Y=df[\"weight\"]\n",
    "degree=3\n",
    "weights = np.polyfit(X,Y, degree) \n",
    "model = np.poly1d(weights) \n",
    "results = smf.ols(formula='Y~ model(X)',data=df).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can plot our fitted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linepoints = np.linspace(min(X), max(X), 100)\n",
    "plt.scatter(X,Y)\n",
    "plt.plot(linepoints, model(linepoints))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Selling ice-creams\n",
    "\n",
    "Make sure the .csv file containing M. Gelato's sales is in your current directory. Start by loading the document and looking at his header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gelato=pd.read_csv(\"Gelato_Times_Sales.csv\")\n",
    "Gelato.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scatter plot: plot M. Gelato's sales as a function of time in a scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"Times\",y=\"Sales\",data=Gelato)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Polynomial regression: d=1\n",
    "    1. Using the statsmodels code in Part 2 Question 2 above, fit a polynomial of degree d=1 to the data. \n",
    "    2. Obtain the summary of your regression. What is the value of R^2? Adjusted R^2?\n",
    "    3. Using the code in Part 2 Question 2, plot the datapoints with the polynomial fit graphed on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **Polynomial regression: larger d**: \n",
    "    1. Repeat the steps above for increasing values of d (for example, for d=2, d=3, d=4, d=5). Write down the values of R^2 you obtain. What happens to R^2? What happens to your fit?\n",
    "    2. What is the smallest degree for which the adjusted R^2 is equal to 1?\n",
    "    3. Consider the polynomial you obtain in this case. Why is R^2 equal to 1 in your opinion?\n",
    "    4. Do you think this fit is a good prediction of what will happen next year? What is the issue here? How would you propose to counterbalance it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Evaluating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn to systematically evaluating a model, based on predictive power. For this, we will split the dataset into training and testing. We then run the polynomial regression on the training set, and see how good it performs the data from the testing set (important: this is data that our model doesn't see when training!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=Gelato[[\"Times\"]]\n",
    "Y=Gelato[[\"Sales\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into training and testing. 25% for testing is quite common. We also fix the `random_state` to make our model reproducible (while it's good practice to be reproducible, you should always check that your results are not dependent on the random state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.25,random_state = 105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start with choosing some degree, here 3, and running the polynomial regression (we use `sklearn` because making predictions is easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree=3\n",
    "poly = PolynomialFeatures(degree)\n",
    "X_poly = poly.fit_transform(trainX)\n",
    "polyreg = LinearRegression()\n",
    "polyreg.fit(X_poly, trainY) # try and find coefficients c1+c2*x+x3*x^2+... via linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now predict the sales on the testing set. For that, we also need to transform our `testX` to fit into the polynomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX_poly = poly.fit_transform(testX)\n",
    "testY_pred = polyreg.predict(testX_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the predictions to see how far they deviate from the actuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,Y)\n",
    "plt.scatter(testX,testY_pred,color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More systematically, we can measure the (root) mean difference between actual values in the testing set and predicted values (we take the square root at the end, to have orders of magnitude related to the sales quantities):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(testY,testY_pred)**(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Finding a good model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a validation set, we run through all the different degrees and find the smallest RMSE. We should then test the final model on a (completely different) testing set - we omit this here, as the size of the dataset is quite small, but we get back to that in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, validateX, trainY, validateY = train_test_split(X, Y, test_size=0.25,random_state = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = []\n",
    "for degree in range(1,12):\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_poly = poly.fit_transform(trainX)\n",
    "    polyreg = LinearRegression()\n",
    "    polyreg.fit(X_poly, trainY)\n",
    "    validateX_poly = poly.fit_transform(validateX)\n",
    "    validateY_pred = polyreg.predict(validateX_poly)\n",
    "    rmse.append(mean_squared_error(validateY,validateY_pred)**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that a degree of 3 would have led to the best RMSE. But keep in mind that we only have very limited data here. If we end up choosing this model, we should evaluate it on a testing set, still!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtvc_env",
   "language": "python",
   "name": "dtvc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
