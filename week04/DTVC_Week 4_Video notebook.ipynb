{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "badf6b13",
   "metadata": {},
   "source": [
    "# Using the Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f922d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc7a828",
   "metadata": {},
   "source": [
    "With the Twitter API we can access most of Twitterâ€™s functionality from within Python (that means both reading **and** writing Tweets, or finding out about users and trends). The package of choice is *Tweepy*, which deals with all the messy details.\n",
    "\n",
    "To access the Twitter API, you need to be authenticated. Hence, every request has to come with authentication information. To get this information in the first place, we need to generate our own credentials with a Developer Account:\n",
    "\n",
    "1. Go to the <a href=https://developer.twitter.com/en>Twitter Developer Site</a> and apply for a Developer Account (you will need a Twitter account for this).\n",
    "2. Create an application (e.g., \"My_first_application\"). Credentials and limits are per application, not per account.\n",
    "3. Once you have created your application, you can transfer your consumer API key and secret, as well as your app access key and secret to the Python code below (see also https://developer.twitter.com/en/docs/basics/authentication/overview/oauth)\n",
    "\n",
    "You can directly add your data as a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_API_KEY = ''\n",
    "CONSUMER_API_SECRET = ''\n",
    "ACCESS_KEY = ''\n",
    "ACCESS_SECRET = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b868c",
   "metadata": {},
   "source": [
    "We are also not allowed to request too many Tweets at the same time. There are per-day limits, as well as \"rate limits\" for 15-minute blocks. If you exceed your limits, you **will** get blocked for some time. For detailed information on the limits, check out https://developer.twitter.com/en/docs/twitter-api/v1/tweets/timelines/overview and https://developer.twitter.com/en/docs/rate-limits.\n",
    "In many cases, we can use the functionality of Tweepy to automatically delay calls in order to wait on the rate limit - but be aware that this doesn't always work, and we may need to manually add timeouts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb87e1",
   "metadata": {},
   "source": [
    "We are now ready to create our verified interface (automatically waiting on our rate limit as necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bb13a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(CONSUMER_API_KEY, CONSUMER_API_SECRET)\n",
    "auth.set_access_token(ACCESS_KEY, ACCESS_SECRET)\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1060fbf9",
   "metadata": {},
   "source": [
    "Let's download some tweets! Note that the API only allows you to download tweets based on general queries from the past 9 days. If you want to download older tweets, you will need to dowload the tweets of a particular account (see below). For simplicity, we will focus on the last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84078d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_day = datetime.utcnow() - timedelta(days=7)\n",
    "start_day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f19551d",
   "metadata": {},
   "source": [
    "We can make a simple query such as `q=\"bayes\"`. However, can you see why this could lead to problems?\n",
    "\n",
    "Luckily, we can simply combine keywords with `OR` and `AND`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d6199",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for tweet in tweepy.Cursor(api.search,q=\"bayes AND business AND school\",\n",
    "                           include_rts=True,lang=\"en\",since=str(start_day.date())).items():\n",
    "    tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6396c743",
   "metadata": {},
   "source": [
    "Here, `include_rts` specifies whether we are also including retweets, and `lang` specifies the language of tweets we request. Let's take a look at our tweets, as well as some of the basic information about them. You can find details about the tweet objects at https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc51190",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    print(\"Created at: \" + str(tweet.created_at))\n",
    "    print(\"User: \" + tweet.user.screen_name)\n",
    "    print(\"Followers: \" + str(tweet.user.followers_count))\n",
    "    print(\"Content: \" + tweet.text)\n",
    "    print(\"---------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3d2b2",
   "metadata": {},
   "source": [
    "# Conjoint analysis (based on rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af198bd7",
   "metadata": {},
   "source": [
    "This exercise is based on Miller, 2015, \"Marketing Data Science: Modeling Techniques in Predictive Analytics with R and Python\".\n",
    "\n",
    "We exemplify here a simple conjoint analysis used to measure how consumers derive utility from certain product attributes (which helps to build a picture of consumer preferences). Conjoint analysis is a key staple of marketing research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c03a983",
   "metadata": {},
   "source": [
    "**Loading the relevant packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f74266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.formula.api import ols\n",
    "from patsy.contrasts import Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17eec45",
   "metadata": {},
   "source": [
    "**Loading the data**\n",
    "\n",
    "The CSV file can be found in the book's Github page at https://github.com/mtpa/mds/blob/master/MDS_Chapter_1/mobile_services_ranking.csv. Note that you cannot use the link directly, rather, you have to get the raw data link (click the button that says \"Raw\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "conjoint_data_frame = pd.read_csv('https://raw.githubusercontent.com/mtpa/mds/master/MDS_Chapter_1/mobile_services_ranking.csv')\n",
    "conjoint_data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358eb305",
   "metadata": {},
   "source": [
    "Let's also get rid of these quotation marks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8aad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conjoint_data_frame = conjoint_data_frame.replace('\"', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ff18d",
   "metadata": {},
   "source": [
    "**Regression with sum contrasts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002e7eb",
   "metadata": {},
   "source": [
    "In a conjoint analysis, we are essentially regressing a ranking, rating, or choice on the the different attributes of the offering (with many different specifications, depending on the question we want an answer to and the survey design). Usually when we perform regressions, we use a Dummy/Treatment coding of categorical variables. That is, one level (say `samsung == 'Samsung NO'`) is the \"baseline\" represented by the intercept, and the effect we measure for any other level (say `samsung == 'Samsung YES'`) is **relative** to that baseline.\n",
    "\n",
    "When using sum contrasts, we measure the main effect of each level of a categorical variable. The baseline, or intercept, now is a hypothetical one: it is formed such that the coefficients of each categorical variable sum up to 0. In practice, if we have a categorical variable that takes two different levels (say the example above), and the effect (or regression coefficient) of level `'Samsung YES'` would be 0.4 \"relative to the baseline\", we would now get a coefficient of 0.2 for `'Samsung YES'` and a coefficient of -0.2 for `'Samsung NO'`. For all intents and purposes, we are centering categorical variables.\n",
    "\n",
    "In Python, we use `patsy.contrasts`. Note in the below code that we don't want to have `ranking` be part of the explanatory variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ca4f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attributes = conjoint_data_frame.columns.tolist()[:-1]\n",
    "formula = 'ranking ~' + ''.join([' C(' + str(attribute) + ', Sum) +' for attribute in attributes])\n",
    "print(\"Without adjustment: \" + formula)\n",
    "formula = formula[:-2]\n",
    "print(\"With adjustment: \" + formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2186f22",
   "metadata": {},
   "source": [
    "Now, we can define a linear regression model as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f937d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols(formula, data = conjoint_data_frame).fit()\n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff77e4b2",
   "metadata": {},
   "source": [
    "**Building part-worth information**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6cb13",
   "metadata": {},
   "source": [
    "To get the part worths, we first need to collect the coefficients of each attribute's levels. We also need to capture the coefficient range to evaluate the importance of attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b8dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_name = []\n",
    "part_worth = []\n",
    "part_worth_range = []\n",
    "for attribute in attributes:\n",
    "    current_levels = sorted(set(conjoint_data_frame[attribute]))\n",
    "    current_levels_remainder_at_end = []\n",
    "    remainder = ''\n",
    "    current_part_worth = []\n",
    "    for level in current_levels:\n",
    "        indicator = 'C('+ attribute +', Sum)[S.'+level+']'\n",
    "        if indicator in model.params:\n",
    "            current_levels_remainder_at_end.append(level)\n",
    "            current_part_worth.append(model.params['C('+ attribute +', Sum)[S.'+level+']'])\n",
    "        else:\n",
    "            remainder = level\n",
    "    current_levels_remainder_at_end.append(remainder)\n",
    "    current_part_worth.append((-1) * sum(current_part_worth))  \n",
    "    part_worth_range.append(max(current_part_worth) - min(current_part_worth))  \n",
    "    part_worth.append(current_part_worth) \n",
    "    level_name.append(current_levels_remainder_at_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e218cdc",
   "metadata": {},
   "source": [
    "Using the range of coefficients, we define the importance of attributes: the larger the range (relative to the other ranges), the larger the importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd9c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_importance = []\n",
    "for att_no in range(len(attributes)):\n",
    "    attribute_importance.append(round(100 * (part_worth_range[att_no] / sum(part_worth_range)),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488fd7b7",
   "metadata": {},
   "source": [
    "**Creating a spine chart of preferences**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf187c7b",
   "metadata": {},
   "source": [
    "Let's look at the attributes in turn, their importance, as well as the coefficients of the individual levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98075a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for att_no in range(len(attributes)):\n",
    "    print('\\nAttribute:', attributes[att_no])\n",
    "    print('    Importance:', attribute_importance[att_no])\n",
    "    print('    Level Part-Worths')\n",
    "    for level in range(len(level_name[att_no])):\n",
    "        print('       ',level_name[att_no][level], part_worth[att_no][level])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a9f7c",
   "metadata": {},
   "source": [
    "Python doesn't have great in-built functionality to create a spine chart of part-worths such as what you find in R, but we can get a start with a `seaborn barplot`. For this, we need to make sure that it differentiates the levels of start-up and monthly costs, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf3e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for att_no in range(len(level_name)):\n",
    "    for level_no in range(len(level_name[att_no])):\n",
    "        if level_name[att_no][level_no].startswith('$'):\n",
    "            level_name[att_no][level_no] = attributes[att_no] + ' ' + level_name[att_no][level_no]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16222284",
   "metadata": {},
   "source": [
    "Next, to create the barplot in the right order, the easiest is if we combine all the relevant information in a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({ 'attribute' : len(part_worth[0])*[attributes[0]],\n",
    "                    'importance' : len(part_worth[0])*[attribute_importance[0]],\n",
    "                    'values' : level_name[0],\n",
    "                    'part_worths' : part_worth[0]})\n",
    "for att_no in range(1,len(attributes)):\n",
    "    for item in range(len(part_worth[att_no])):\n",
    "        df = df.append({ 'attribute' : attributes[att_no],\n",
    "                        'importance' : attribute_importance[att_no],\n",
    "                        'values' : level_name[att_no][item],\n",
    "                        'part_worths' : part_worth[att_no][item]},ignore_index=True)\n",
    "df = df.sort_values(['importance', 'attribute', 'values'], ascending=[False,True,True])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858304b0",
   "metadata": {},
   "source": [
    "Finally, we can take a look at the part-worths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df,x=\"part_worths\", y=\"values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f2323d",
   "metadata": {},
   "source": [
    "How would things look like with your own rankings?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtvc_env",
   "language": "python",
   "name": "dtvc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
