{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ffa4b72",
   "metadata": {},
   "source": [
    "# Social Media APIs and Marketing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e39491",
   "metadata": {},
   "source": [
    "## 1. Google trends to predict Covid cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289fd0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pytrends.request import TrendReq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6cf5eb",
   "metadata": {},
   "source": [
    "*Google Flu Trends* was a web service operated by Google. It provided estimates of influenza activity for more than 25 countries. By aggregating Google Search queries, it attempted to make accurate predictions about flu activity. This project was first launched in 2008 by *Google.org* to help predict outbreaks of flu. *Google Flu Trends* stopped publishing current estimates on 9 August 2015. We can use the open access tool *Google Trends* to build a similar engine that can predict the daily number of Covid cases. Here, we take a first step, by identifying patterns between searches and cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184d8370",
   "metadata": {},
   "source": [
    "**Loading a data set of daily Covid cases**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a8f80",
   "metadata": {},
   "source": [
    "We start by analyzing daily Covid data with Google trends. Before getting into the Google API, we take data about daily cases from \"Our World in Data\", which is freely available <a href='https://github.com/owid/covid-19-data/tree/master/public/data'>here</a> (the site also recommends the below stable link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2dc2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://covid.ourworldindata.org/data/owid-covid-data.csv')\n",
    "df.fillna(0,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97281b5d",
   "metadata": {},
   "source": [
    "As always when working with dates, we want to make sure that our data is actually recognized as such (and not just a string), so we convert the `date` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778be1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac6e649",
   "metadata": {},
   "source": [
    "Let's start with a specific location and time: Covid cases in the US between April 1, 2020, and December 1, 2020 (some more on the choice of date later). Therefore, continue by creating a new dataframe containing only the relevant data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12eb48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df = df[(df['date'] <= datetime(2020,12,1)) & (df['date'] >= datetime(2020,4,1)) & (df['iso_code'] == 'USA')]\n",
    "us_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ed965",
   "metadata": {},
   "source": [
    "**Using Google Trends to find out about searches**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361980d6",
   "metadata": {},
   "source": [
    "`pytrend`'s `TrendReq` connects us directly with the Google Trend API. We use `build_payload` to establish what we are looking for (keywords, location, timeframe, etc.), and can then call up different data, such as interest over time, interest by region, related topics, related queries, trending searches, top charts, suggestions, or categories. The full documentation is <a href=\"https://github.com/GeneralMills/pytrends\">here</a>.\n",
    "\n",
    "Let's start by getting `interest_over_time()` for the search \"can't smell\" during the same time frame as above. Note that, if you specify a time frame longer than 270 days, the API will return weekly results instead of daily ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c665bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytrend = TrendReq()\n",
    "pytrend.build_payload(kw_list = [\"can't smell\"],timeframe='2020-04-01 2020-12-01', geo=\"US\")\n",
    "smell_trends = pytrend.interest_over_time()\n",
    "smell_trends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bec1bb9",
   "metadata": {},
   "source": [
    "Note that `interest_over_time()` already generates a data frame. However, this is indexed by date, which can make merging a bit tricky. So instead of using that data frame, we create a new one with the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7272c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "smell_trends = pd.DataFrame(data = {'date': smell_trends.index.tolist(), 'search': smell_trends[\"can't smell\"].tolist()})\n",
    "smell_trends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b2f11",
   "metadata": {},
   "source": [
    "Let's now take a look at the searches over time. Note that the \"interest\" returned by our query is relative to the peak interest within the query timeframe (so the maximum will always be 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot( x = 'date',\n",
    "             y = 'search',\n",
    "             data = smell_trends,\n",
    "             label = 'daily hits', color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e697bb4",
   "metadata": {},
   "source": [
    "To get a better idea of what is happening, let's add a rolling weekly average (we can use `column.rolling(k)` where `k` is the number of entries). Note that this creates some `NA` values, because there is no rolling average for the first 6 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a399871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "smell_trends['rolling_avg' ] = smell_trends.search.rolling(7).mean()\n",
    "smell_trends = smell_trends.fillna(0)\n",
    "sns.scatterplot( x = 'date',\n",
    "             y = 'search',\n",
    "             data = smell_trends,\n",
    "             label = 'daily hits', color=\"blue\")\n",
    "sns.lineplot( x = 'date',\n",
    "             y = 'rolling_avg',\n",
    "             data = smell_trends,\n",
    "             label = 'rolling_avg', color=\"orange\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a16ff8",
   "metadata": {},
   "source": [
    "In order to run a linear regression, all the data needs to be in a single dataframe. Consequently, we need to combine the Google trends data with the Covid data. This can be done seamlessly by merging the two dataframes into a new one using the command `.merge()` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b04588",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(us_df,smell_trends,on=\"date\")\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d9f7f0",
   "metadata": {},
   "source": [
    "**Combining cases and searches graphically**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6b0eca",
   "metadata": {},
   "source": [
    "Next, we plot the two time series together, to see if we can spot a pattern. Note that the scales are vastly different, so it pays off to create a graph with multiple scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6651017",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=combined_df.date, y=combined_df.new_cases, color = \"red\")\n",
    "ax.set_ylabel(\"cases\",color=\"red\",fontsize=14)\n",
    "ax2 = plt.twinx()\n",
    "sns.scatterplot(x=combined_df.date, y=combined_df.search, color = \"blue\")\n",
    "ax2.set_ylabel(\"search\",color=\"blue\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f010a0e8",
   "metadata": {},
   "source": [
    "**Estimating the relationship between searches and cases**\n",
    "\n",
    "We can start with a simple regression of the daily cases on the searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470d663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(combined_df.search)\n",
    "model = sm.OLS(combined_df.new_cases, X)\n",
    "print(model.fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d06f2e8",
   "metadata": {},
   "source": [
    "We can probably find a better model than that. For example, there is a lot of noise, so we may want to smooth out the data. Let's try using the rolling weekly average of cases that we added in before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541fbe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(combined_df.rolling_avg)\n",
    "model = sm.OLS(combined_df.new_cases, X)\n",
    "print(model.fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956eef7f",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "Try increasing the predictive power of this method even further. There could be a lag between intial symptoms and an eventual test - we can created lagged variables using `column.shift(k)` where `k` is the number of observations by which the column is shifted downward (e.g., if `k=7`, the \"search\" data from day 1 is now in the new column at day 8). Of course, there are no values to shift in anywhere before observation `k`, so don't forget to deal with `NA` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93805b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['shifted_1_week'] = combined_df.search.shift(7)\n",
    "combined_df['shifted_2_weeks'] = combined_df.search.shift(14)\n",
    "combined_df['shifted_3_weeks'] = combined_df.search.shift(21)\n",
    "combined_df = combined_df.fillna(0)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aaefd4",
   "metadata": {},
   "source": [
    "Run the model with the right columns for the `X` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a58d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(combined_df[['rolling_avg','shifted_1_week','shifted_2_weeks','shifted_3_weeks']])\n",
    "model = sm.OLS(combined_df.new_cases, X)\n",
    "print(model.fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42448c1c",
   "metadata": {},
   "source": [
    "What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee357f",
   "metadata": {},
   "source": [
    "### Exercise 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebef5f0a",
   "metadata": {},
   "source": [
    "Let's explore another country: check both of the search terms \"smell\" and \"fever\" for the United Kingdom (`geo=\"GB\"`), for the timeframe January 1, 2020 to May 1, 2020 (`'2020-01-01 2020-05-01'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e8c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytrend = TrendReq()\n",
    "pytrend.build_payload(kw_list = [\"smell\",\"fever\"],timeframe='2020-01-01 2020-05-01', geo=\"GB\")\n",
    "trends = pytrend.interest_over_time()\n",
    "trends = pd.DataFrame(data = {'date': trends.index.tolist(),\n",
    "                              'search_smell': trends[\"smell\"].tolist(),\n",
    "                              'search_fever': trends[\"fever\"].tolist()})\n",
    "trends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e55da1",
   "metadata": {},
   "source": [
    "Try to plot the patterns of both columns in a single plot (e.g., using `seaborn`). What about differences do you see in those patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef41e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot( x = 'date',\n",
    "                 y = 'search_smell',\n",
    "                 data = trends,\n",
    "                 label = 'daily hits smell')\n",
    "sns.scatterplot( x = 'date',\n",
    "                 y = 'search_fever',\n",
    "                 data = trends,\n",
    "                 label = 'daily hits fever')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d91eee",
   "metadata": {},
   "source": [
    "### Discussion point: what could this type of data be helpful for in a buisness setting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4592bee",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b104d8b7",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3ca6d9",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5427f23",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166737e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaedf41",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6592c4",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7b96ca",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399a11eb",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48efc519",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611e4dd",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d6ce6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ddeca",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b38c0b6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c18890",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf7a1e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe5fe73",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a9e030",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e9437",
   "metadata": {},
   "source": [
    "## 2. Measuring engagement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a4e4dc",
   "metadata": {},
   "source": [
    "Let's use our newfound skills with the Google trends API to measure enagagement following a marketing campaign. The company Red Bull spends a large sum of money on its Formula 1 team, in order to market its brand. But it also does lots of other marketing activities. Because most people don't go to the supermarket to buy a Red Bull drink after watching a Formula 1 event, it can be quite tricky to associate sales with different marketing campaigns. But social media allows us to capture consumer reactions and engagement in real time! So if we know that consumer engagement leads to more sales (at least, on the long term), it can be extremely valuable to measure engagement following marketing campaigns.\n",
    "\n",
    "Our question will be twofold:\n",
    "1. Does Red Bull create engagement with its Formula 1 expenditures?\n",
    "2. Does the Pilot matter? I.e., is engagement related to success?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91744533",
   "metadata": {},
   "source": [
    "We start by pulling Google trends data for the search term \"red bull\" between March and yesterday (the Formula 1 season started at the end of March)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ad42a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytrend = TrendReq()\n",
    "pytrend.build_payload(kw_list = [\"red bull\"],timeframe='2021-03-01 2021-10-28')\n",
    "trends = pytrend.interest_over_time()\n",
    "trends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90998d6",
   "metadata": {},
   "source": [
    "Note: Google Trends only gives us data about searches up to approx. three days ago."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629187d0",
   "metadata": {},
   "source": [
    "As before, we want to clean up the dataset a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trendsdf = pd.DataFrame(data = {'date': trends.index.tolist(),\n",
    "                                'search': trends[\"red bull\"].tolist()})\n",
    "trendsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97aad92",
   "metadata": {},
   "source": [
    "Don't forget to convert the date string to an actual Date object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "trendsdf['date'] = pd.to_datetime(trendsdf['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f969313",
   "metadata": {},
   "source": [
    "Next, we take a look at the daily search hits (recall that the maximum will always be 100 in any Google Trends query):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f84c372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.scatterplot( x = 'date',\n",
    "             y = 'search',\n",
    "             data = trendsdf,\n",
    "             label = 'daily hits', color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdb7cd6",
   "metadata": {},
   "source": [
    "We have some data on the searches. Now, we can add race data into the mix (you can find the csv on Moodle):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "racingdf = pd.read_csv('red_bull_race_results.csv')\n",
    "racingdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e965d",
   "metadata": {},
   "source": [
    "Make sure that our dates are actual dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7520bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "racingdf['date'] = pd.to_datetime(racingdf['date'], format=\"%d.%m.%y\")\n",
    "racingdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d311c1",
   "metadata": {},
   "source": [
    "The dataset gives the positioning of the two Red Bull pilots, Sergio Pérez and Max Verstappen. A missing value indicates that the pilot did not finish the race."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62d7d3",
   "metadata": {},
   "source": [
    "Similar to before, we can merge the two data frames by date, in order to understand whether race day implies a high number of searches. For simplicity, we can take only one extra column of `racingdf`, to see whether the date exists or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ed27f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.merge(trendsdf, racingdf[['date','perez']], how='left',on='date')\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9114132",
   "metadata": {},
   "source": [
    "Whereever there is a missing value in the `perez` column, there was no race on the day. We can adjust our dataframe accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['raceday'] = temp['perez'].notna()\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7be28c",
   "metadata": {},
   "source": [
    "It's time to plot our merged result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97777415",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot( x = 'date',\n",
    "             y = 'search',\n",
    "             data = temp,\n",
    "            hue=\"raceday\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857074d3",
   "metadata": {},
   "source": [
    "A first look at the data seems to indicate that there is support for the hypothesis that the Formula 1 marketing leads to customer engagement. Of course, more work is needed to establish robust evidence.\n",
    "\n",
    "However, we will turn to the second question instead (whether the pilots matter). For this, we need to first clean the racing dataset.\n",
    "\n",
    "Since disqualification is often a topic of intense interest in Formula 1, we create an additional column to measure whether a driver has not completed the race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7a751",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "racingdf['perez_out'] = racingdf['perez'].isna().astype(int)\n",
    "racingdf['verstappen_out'] = racingdf['verstappen'].isna().astype(int)\n",
    "racingdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b1fd3",
   "metadata": {},
   "source": [
    "There is one special case, which requires contextual knowledge: on 2021-06-06, Verstappen did not finish the race and was only placed because he had completed more than 90%. For consistency, we may consider putting a 1 in the column `'verstappen_out'` here (this is a typical case in which you want to check for consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "racingdf.loc[racingdf['date'] == '2021-06-06','verstappen_out'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28840923",
   "metadata": {},
   "source": [
    "We then remove the `NAs` by replacing them with the worst result (this is of course just one possible choice and requires further analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c02428",
   "metadata": {},
   "outputs": [],
   "source": [
    "racingdf['perez'] = racingdf['perez'].fillna(racingdf['perez'].max())\n",
    "racingdf['verstappen'] = racingdf['verstappen'].fillna(racingdf['verstappen'].max())\n",
    "racingdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf40080",
   "metadata": {},
   "source": [
    "Next, we want to find out the search results around the race days. We start with an example, by taking the first race day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57af81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_date = racingdf.loc[0,'date']\n",
    "race_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41d9a74",
   "metadata": {},
   "source": [
    "We now define a time difference, between the race date and the date of searches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71579ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trendsdf['difftime'] = trendsdf['date']-race_date\n",
    "trendsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae50ff",
   "metadata": {},
   "source": [
    "We want this time difference to be in days, but as a number. For this, we use the `.days` attribute of the time difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4b598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trendsdf['difftime'] = [diff.days for diff in trendsdf['difftime']]\n",
    "trendsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf65ace7",
   "metadata": {},
   "source": [
    "Finally, we want to focus on the trends results 3 days before and after the race:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "racetrends = trendsdf[abs(trendsdf['difftime']) <= 3]\n",
    "racetrends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a4313b",
   "metadata": {},
   "source": [
    "Because overall trends may vary all the time for a number of factors, we focus on measuring the race day searches compared to the lowest search number in the week around the race:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(racetrends['search'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd3911",
   "metadata": {},
   "outputs": [],
   "source": [
    "racetrends[racetrends['difftime']==0].iloc[0]['search']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dded94",
   "metadata": {},
   "outputs": [],
   "source": [
    "racetrends[racetrends['difftime']==0].iloc[0]['search']/min(racetrends['search'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b7d42",
   "metadata": {},
   "source": [
    "Let's put it all together and create an `\"engagement\"` column in our `racingdf`. In particular, we go through all the race dates and find the engagement (race day searches relative to lowest search number in the week)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda31304",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_engagement = []\n",
    "for i in range(len(racingdf)):\n",
    "    race_date = racingdf.iloc[i,]['date']\n",
    "    trendsdf['difftime'] = trendsdf['date']-race_date\n",
    "    trendsdf['difftime'] = [diff.days for diff in trendsdf['difftime']]\n",
    "    racetrends = trendsdf[abs(trendsdf['difftime']) <= 3]\n",
    "    if len(racetrends[racetrends['difftime']==0]) > 0:\n",
    "        relative_engagement.append( racetrends[racetrends['difftime']==0].iloc[0]['search']/min(racetrends['search']) )\n",
    "    else:\n",
    "        relative_engagement.append(np.nan)\n",
    "racingdf['engagement'] = relative_engagement\n",
    "racingdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f10316",
   "metadata": {},
   "source": [
    "Finally, recall that Google Trends only gives us search data until three days ago. Hence, we should cut out the latest race to avoid bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed52a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "racingdf_short = racingdf.drop(racingdf.tail(1).index)\n",
    "racingdf_short"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc718dd7",
   "metadata": {},
   "source": [
    "Now that we have the data, we can try to see what effects the placements have on engagement. Of course, we also analyze the effect of a driver not finishing the race:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32ee555",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = racingdf_short.drop(['date', 'engagement'], axis=1)\n",
    "Y = racingdf_short.engagement\n",
    "X = sm.add_constant(X)\n",
    "lm = sm.OLS(Y,X).fit()\n",
    "print (lm.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69d894f",
   "metadata": {},
   "source": [
    "### Discussion point: Can you interpret these results? What do they indicate in terms of our second question? Why are the effect sizes different for the different drivers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286272e1",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe260c",
   "metadata": {},
   "source": [
    "## 3. Interlude: Using Twitter data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653de47",
   "metadata": {},
   "source": [
    "It might be useful to also see what users are tweeting about regarding the races. Let's take a look at the Twitter API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f922d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc7a828",
   "metadata": {},
   "source": [
    "With the Twitter API we can access most of Twitter’s functionality from within Python (that means both reading **and** writing Tweets, or finding out about users and trends). The package of choice is *Tweepy*, which deals with all the messy details.\n",
    "\n",
    "To access the Twitter API, you need to be authenticated. Hence, every request has to come with authentication information. To get this information in the first place, we need to generate our own credentials with a Developer Account:\n",
    "\n",
    "1. Go to the <a href=https://developer.twitter.com/en>Twitter Developer Site</a> and apply for a Developer Account (you will need a Twitter account for this).\n",
    "2. Create an application (e.g., \"My_first_application\"). Credentials and limits are per application, not per account.\n",
    "3. Once you have created your application, you can transfer your consumer API key and secret, as well as your app access key and secret to the Python code below (see also https://developer.twitter.com/en/docs/basics/authentication/overview/oauth)\n",
    "\n",
    "You can directly add your data as a string like this:\n",
    "```\n",
    "CONSUMER_API_KEY = 'COPY STRING HERE'\n",
    "CONSUMER_API_SECRET = 'COPY STRING HERE'\n",
    "ACCESS_KEY = 'COPY STRING HERE'\n",
    "ACCESS_SECRET = 'COPY STRING HERE'\n",
    "```\n",
    "\n",
    "So that I can share my code without everyone using my credentials (which would probably lead to me being blocked by Twitter), I'm instead reading the data from a csv here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_access = pd.read_csv('API_access.csv',delimiter=';')\n",
    "CONSUMER_API_KEY = api_access[api_access['api'] == 'twitter_consumer_api_key']['key'].tolist()[0]\n",
    "CONSUMER_API_SECRET = api_access[api_access['api'] == 'twitter_consumer_api_secret']['key'].tolist()[0]\n",
    "ACCESS_KEY = api_access[api_access['api'] == 'twitter_access_key']['key'].tolist()[0]\n",
    "ACCESS_SECRET = api_access[api_access['api'] == 'twitter_access_secret']['key'].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b868c",
   "metadata": {},
   "source": [
    "We are also not allowed to request too many Tweets at the same time. There are per-day limits, as well as \"rate limits\" for 15-minute blocks. If you exceed your limits, you **will** get blocked for some time. For detailed information on the limits, check out https://developer.twitter.com/en/docs/twitter-api/v1/tweets/timelines/overview and https://developer.twitter.com/en/docs/rate-limits.\n",
    "In many cases, we can use the functionality of Tweepy to automatically delay calls in order to wait on the rate limit - but be aware that this doesn't always work, and we may need to manually add timeouts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb87e1",
   "metadata": {},
   "source": [
    "We are now ready to create our verified interface (automatically waiting on our rate limit as necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bb13a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(CONSUMER_API_KEY, CONSUMER_API_SECRET)\n",
    "auth.set_access_token(ACCESS_KEY, ACCESS_SECRET)\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1060fbf9",
   "metadata": {},
   "source": [
    "Let's download some tweets! Note that the API only allows you to download tweets based on general queries from the past 9 days. If you want to download older tweets, you will need to dowload the tweets of a particular account (see below). For simplicity, we will focus on the last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84078d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_day = datetime.utcnow() - timedelta(days=7)\n",
    "start_day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f19551d",
   "metadata": {},
   "source": [
    "We can make a simple query such as `q=\"bayes\"`. However, can you see why this could lead to problems?\n",
    "\n",
    "Luckily, we can simply combine keywords with `OR` and `AND`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d6199",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for tweet in tweepy.Cursor(api.search,q=\"bayes AND business AND school\",\n",
    "                           include_rts=True,lang=\"en\",since=str(start_day.date())).items():\n",
    "    tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6396c743",
   "metadata": {},
   "source": [
    "Here, `include_rts` specifies whether we are also including retweets, and `lang` specifies the language of tweets we request. Let's take a look at our tweets, as well as some of the basic information about them. You can find details about the tweet objects at https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc51190",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    print(\"Created at: \" + str(tweet.created_at))\n",
    "    print(\"User: \" + tweet.user.screen_name)\n",
    "    print(\"Followers: \" + str(tweet.user.followers_count))\n",
    "    print(\"Content: \" + tweet.text)\n",
    "    print(\"---------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185fb9b6",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "Repeat the query above, searching for tweets with the hashtag #bayes without date limitations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fad35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for tweet in tweepy.Cursor(api.search,q=\"#bayes\",\n",
    "                           include_rts=True,lang=\"en\").items():\n",
    "    tweets.append(tweet)\n",
    "    \n",
    "for tweet in tweets:\n",
    "    print(\"Created at: \" + str(tweet.created_at))\n",
    "    print(\"User: \" + tweet.user.screen_name)\n",
    "    print(\"Followers: \" + str(tweet.user.followers_count))\n",
    "    print(\"Content: \" + tweet.text)\n",
    "    print(\"---------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d9dac6",
   "metadata": {},
   "source": [
    "## 4. Finding Red bull tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5644f",
   "metadata": {},
   "source": [
    "Let's go back to Red Bull, and search for tweets with the hash tag `\"#redbull\"`.\n",
    "\n",
    "Note: in principle, the `include_rts=False` option should exclude any retweets. In practice, however, there seems to be some issues in how `tweepy` performs this exclusion. Hence, we make sure manually to only capture original tweets, using the keyworkd `\"-filter:retweets\"` in our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9759cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for tweet in tweepy.Cursor(api.search,q=\"#redbull -filter:retweets\",\n",
    "                           lang=\"en\",since=str(start_day.date())).items():\n",
    "    tweets.append(tweet)\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa37c10",
   "metadata": {},
   "source": [
    "A side note: When requesting tweets in this manner, the API will cut anything beyond 140 characters. That means, even if we search for tweets with #redbull, the tweet we receive may not contain the hashtag. However, we can\n",
    "- \"Hydrate\" tweets at any time, using just their ID (i.e. request the full text). You can thus use only the tweet ID to share your data\n",
    "- add the parameter `tweet_mode='extended'` to our `tweepy.Cursor()` call. In this case, returned tweets no longer have a `.text` attribute, but a `.full_text` attribute "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802f4a6d",
   "metadata": {},
   "source": [
    "Let's start having a look at Tweeter demographics. Where are tweeters from (we only consider accounts with locations)? Remember, that the <a href=\"https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet\">Developer Platform</a> has all the relevant information about tweet objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe633f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweets[0]\n",
    "tweet.user.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ece748",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_loc = [tweet.user.location for tweet in tweets if tweet.user.location != \"\"]\n",
    "len(tweet_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b74444",
   "metadata": {},
   "source": [
    "We will consider only locations that have at least 5 tweets emerging from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6795c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_loc_df = pd.DataFrame(tweet_loc,columns=['location'])\n",
    "tweet_loc_df = tweet_loc_df.groupby('location')['location'].count().reset_index(name='count')\n",
    "tweet_loc_df = tweet_loc_df[tweet_loc_df['count'] >= 5]\n",
    "tweet_loc_df.sort_values(by='count',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d615a5c6",
   "metadata": {},
   "source": [
    "Do you see a possible problem here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eece30",
   "metadata": {},
   "source": [
    "Let's now take a look at when the Tweets where sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.created_at.date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad2c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.created_at.date().day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed57dc",
   "metadata": {},
   "source": [
    "We can collect the counts per day into a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a30377",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_day = [tweet.created_at.date().day for tweet in tweets]\n",
    "tweet_day_df = pd.DataFrame(tweet_day,columns=['day'])\n",
    "tweet_day_df = tweet_day_df.groupby('day')['day'].count().reset_index(name='count')\n",
    "tweet_day_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4784b796",
   "metadata": {},
   "source": [
    "Let's see this graphically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91ca528",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(y=tweet_day_df['count'], x = tweet_day_df['day'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb74cd",
   "metadata": {},
   "source": [
    "The variation seems familiar, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e122e951",
   "metadata": {},
   "source": [
    "We now want to learn more about the people (and company accounts) that follow Red Bull (as well as about whom they follow other than Red Bull). Let's start with finding some of Red Bull's followers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd70d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_rb = []\n",
    "for follower in tweepy.Cursor(api.followers,\"redbull\").items(5):\n",
    "    followers_rb.append(follower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef84be9",
   "metadata": {},
   "source": [
    "A company like Red Bull has quite some followers and we would run into problems trying to get all at once. But if you really care about collecting everything, the following code times out for 15 minutes after every 3000 names (watch out, this would still run for about 170 hours. It is advisable to check the code in more detail and to add an option for saving lists once in a while):\n",
    "\n",
    "```\n",
    "followers_rb = []\n",
    "users = tweepy.Cursor(api.followers, screen_name=accountvar, count=200).items()\n",
    "while True:\n",
    "    try:\n",
    "        user = next(users)\n",
    "    except tweepy.TweepError:\n",
    "        time.sleep(60*15)\n",
    "        user = next(users)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    followers_rb.append(user.screen_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e213d0a7",
   "metadata": {},
   "source": [
    "Note that followers are saved as \"User\" objects, with their very own attributes, found here: https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/user. The twitter-handle is defined by the `screen_name` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "follower = followers_rb[0]\n",
    "follower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0657c33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "follower.screen_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6615a1e",
   "metadata": {},
   "source": [
    "Can we get other accounts that this person follows? (Twitter defines those as friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933083c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in tweepy.Cursor(api.friends, screen_name=follower.screen_name).items(10):\n",
    "    print(user.screen_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0b8b46",
   "metadata": {},
   "source": [
    "Sometimes, the information is set to private, so we don't know who the person is following. Hence, we need to do some Exception management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9489e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for user in tweepy.Cursor(api.friends, screen_name=follower.screen_name).items(10):\n",
    "        print(user.screen_name)\n",
    "except tweepy.TweepError:\n",
    "    print(\"Follower \" + follower.screen_name + \" does not provide access to their friends.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844b0630",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "Let's combine this for multiple of Red Bull's followers. For the first 5 followers, let's get up 10 of the accounts that they follow each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followers_friends = []\n",
    "for follower in followers_rb:\n",
    "    followers_friends = []\n",
    "    try:\n",
    "        for user in tweepy.Cursor(api.friends, screen_name=follower.screen_name).items(10):\n",
    "            followers_friends.append(user)\n",
    "    except tweepy.TweepError:\n",
    "        print(\"Follower \" + follower.screen_name + \" does not provide access to their friends.\")\n",
    "    print(\"Added \" + str(len(followers_friends)) + \" friends of follower \" + follower.screen_name)\n",
    "    all_followers_friends.append(followers_friends)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5b9a8",
   "metadata": {},
   "source": [
    "It's easy to imagine how we could create a network of accounts, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d931a88",
   "metadata": {},
   "source": [
    "We can also take a look at all the Tweets of Red Bull itself. When looking at an account's Tweets, we do not have to worry about date limits (but there are still restrictions, so let's make sure not to pull too many)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee70bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_rb = []\n",
    "for tweet in tweepy.Cursor(api.user_timeline,id='redbull').items(1000):\n",
    "    tweets_rb.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_rb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1758a92a",
   "metadata": {},
   "source": [
    "## 5. Back to our engagement measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6086d3",
   "metadata": {},
   "source": [
    "Let's try to enrich our `racingdf` using Tweet data. This may help us to answer the second question: What is the role of the pilot?\n",
    "\n",
    "We can only collect tweets by hashtag for a bit more than a week. Hence, I have prepared a full week of tweets every Wednesday after a Formula 1 race in the last few weeks. This is stored as a `pickle` file - a system that allows to directly save arbitrary Python objects outside of our program. Hence, once we call up the pickle file, we get back exactly the variables we saved into it. Since I saved a list of tweets, the return value from `pickle.load(file)` will be a list of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171ac547",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Tweets_210908_211027.txt', 'rb') as file:\n",
    "    tweets = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2fe8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b60ac4",
   "metadata": {},
   "source": [
    "Briefly recall our dataset `racingdf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d32344",
   "metadata": {},
   "outputs": [],
   "source": [
    "racingdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eee0be5",
   "metadata": {},
   "source": [
    "The tweet data only captures the last four races, so we will focus on the dates of those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afb3248",
   "metadata": {},
   "outputs": [],
   "source": [
    "racedates = racingdf.tail(4)['date'].tolist()\n",
    "racedates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88353224",
   "metadata": {},
   "source": [
    "Let's take the first of these race dates, as well as an \"arbitrary\" tweet and compare dates (don't be surprised, this will lead to an error!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b6c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "racedate = racedates[0]\n",
    "tweet = tweets[219]\n",
    "tweet.created_at.date() - racedate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4f1ab8",
   "metadata": {},
   "source": [
    "The problem here is that the date objects used by `tweepy` are not the same as the date objects used by `pandas`! Hence, let's convert the tweet's date with `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b393217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(tweet.created_at.date()) - racedate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee63384",
   "metadata": {},
   "source": [
    "We can convert this time difference into days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbceed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.to_datetime(tweet.created_at.date()) - racedate).days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2624f3",
   "metadata": {},
   "source": [
    "Ok, the tweet comes from the right day. Let's take a look what it is about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ed3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[219].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8365831",
   "metadata": {},
   "source": [
    "The name \"Verstappen\" appears in here. We can, of course, check this automatically with Python (note that we use `.lower()` to avoid issues when comparing different capitalization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df05ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "'verstappen' in tweets[219].text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69642c4e",
   "metadata": {},
   "source": [
    "We can combine the above code to create two new columns: a count of tweets talking about Perez and a count of tweets talking about Verstappen. We first create an empty column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c8bd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "racingdf['tweets_perez'] = 0\n",
    "racingdf['tweets_verstappen'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50efcd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for racedate in racedates:\n",
    "    perez_count = 0\n",
    "    verstappen_count = 0\n",
    "    for tweet in tweets:\n",
    "        if (pd.to_datetime(tweet.created_at.date()) - racedate).days in [0,1]:\n",
    "            if 'perez' in tweet.text.lower():\n",
    "                perez_count += 1\n",
    "            if 'verstappen' in tweet.text.lower():\n",
    "                verstappen_count += 1\n",
    "    racingdf.loc[racingdf['date'] == racedate,\"tweets_perez\"] = perez_count\n",
    "    racingdf.loc[racingdf['date'] == racedate,\"tweets_verstappen\"] = verstappen_count\n",
    "racingdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d6d091",
   "metadata": {},
   "source": [
    "### Discussion point: Can you interpret these results? Why is the number of mentions of Verstappen so high on September 12?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7062af3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fadf64a",
   "metadata": {},
   "source": [
    "**Related Hashtags**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f0b2c",
   "metadata": {},
   "source": [
    "Are people only talking about Red Bull because of Formula 1, or are there other reasons? Answering this may help to answer our first question.\n",
    "\n",
    "We can look directly at the context of the tweets by looking at all its hashtags, without self-processing the text first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2fa1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweets[0]\n",
    "tweet.entities['hashtags']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaaf19e",
   "metadata": {},
   "source": [
    "There are two hashtags here. We focus on the first one, since we already knew the second one has to be here (it was our search term after all). Note the structure of hashtags: It's a list of dictionaries, where the key `'text'` gives the hashtag and the key `'indices'` gives the position within the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945cf763",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.entities['hashtags'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba1e55b",
   "metadata": {},
   "source": [
    "Generally, we want to avoid issues with capitalization by using `.lower()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f9c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.entities['hashtags'][0]['text'].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d806e377",
   "metadata": {},
   "source": [
    "Let's now get all the hashtags from our tweets (except the redbull hashtags):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b60a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "attached_tags = [tweet.entities['hashtags'] for tweet in tweets if tweet.entities['hashtags'] != []]\n",
    "attached_tags_cleaned = [hashtag['text'].lower() for tags in attached_tags for hashtag in tags if hashtag['text'].lower() != 'redbull']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cf30e7",
   "metadata": {},
   "source": [
    "We can convert these into a dataframe, where we count the occurences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed9b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_df = pd.DataFrame(attached_tags_cleaned,columns=['hashtag'])\n",
    "hashtag_df = hashtag_df.groupby('hashtag')['hashtag'].count().reset_index(name='count')\n",
    "hashtag_df.sort_values(by='count',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ea3df5",
   "metadata": {},
   "source": [
    "That gives us an initial idea that, yes, at least around the weeks of Formula 1 races, this is the biggest topic with which Red Bull causes engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b83e3e",
   "metadata": {},
   "source": [
    "\n",
    "**Text analysis**\n",
    "\n",
    "We will next use some basic text analytics tools to find out more about what people have to say (and hereby help us find a better answer to Question 2).\n",
    "\n",
    "We can start with the most used words. This gives a sense of how people are perceiving Red Bull. We can easily split tweets into words using `.split()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03088e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweets[0]\n",
    "tweet.text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f864be",
   "metadata": {},
   "source": [
    "Let's use this to generate a complete list of (lowercase) words (without hashtags):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc6e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [tweet.text for tweet in tweets]\n",
    "word_list = [word.lower() for text in text_list for word in text.split() ]\n",
    "word_list = [word.replace('#','') for word in word_list ]\n",
    "word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aec6a66",
   "metadata": {},
   "source": [
    "Note: if we got the full tweet text, instead of the first 140 characters, we would have to use `tweet.full_text` instead of `tweet.text`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e610cfac",
   "metadata": {},
   "source": [
    "We put these words into a dataframe and count their occurence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc2f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame(word_list,columns=['word'])\n",
    "word_df = word_df.groupby('word')['word'].count().reset_index(name='count')\n",
    "word_df.sort_values(by='count',ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c91600",
   "metadata": {},
   "source": [
    "There is a lot of junk here. One first attempt to clean up this table is to remove all English stopwords (the most common words like \"the\" and \"a\"). Many libraries can do this for us, such as `nltk`. But if we haven't used `nltk` before, we need to download the stopword library first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02929bc2",
   "metadata": {},
   "source": [
    "The English stopwords shouldn't be all too surprising:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af08fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206e7361",
   "metadata": {},
   "source": [
    "We can now remove all the (English) stopwords from the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c4332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = word_df[~(word_df['word'].isin(nltk.corpus.stopwords.words('english')))]\n",
    "word_df.sort_values(by='count',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f469bf47",
   "metadata": {},
   "source": [
    "Beyond counting words, there are fantastic tools out there to analyze sentinments. Usually, we need to start by training a sentiment analyzer. Luckily, `nltk` comes with an in-built pre-trained sentiment analyzer (VADER), purpose-built for analyzing short text on social media (convenient, right?). To use it for the first time, we first have to download its lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ffc1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189b70a",
   "metadata": {},
   "source": [
    "Let's see how it works by handing it a short sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d936b82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"My outlook on life is fantastic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafc791e",
   "metadata": {},
   "source": [
    "The negative, neutral, and positive scores are self-explanatory and numbers here are between $0$ and $1$, with the total adding up to 1. The compound score follows a somewhat complex arithmetic, but it's easy to understand how to use it: it's between $-1$ and $1$, anything $>0$ signifies a positive sentiment, and anything $<0$ signifies a negative sentiment.\n",
    "\n",
    "Can it tell us something about cliché optimists and pessimists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba667c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"The glass is half full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a9585",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"The glass is half empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8988be",
   "metadata": {},
   "source": [
    "What do you think? Does it make sense to rate the first sentence as neutral and the second one as negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb3427",
   "metadata": {},
   "source": [
    "Now let's apply this simple sentiment analysis to our tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14732732",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweets[0]\n",
    "print(tweet.text)\n",
    "sia.polarity_scores(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc1399",
   "metadata": {},
   "source": [
    "We will now go through all tweets, finding the compound score of each tweet and then displaying a histogram of compound scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1a38f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_scores = []\n",
    "for tweet in tweets:\n",
    "    compound_scores.append(sia.polarity_scores(tweet.text)['compound'])\n",
    "sns.histplot(compound_scores,bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e649b23e",
   "metadata": {},
   "source": [
    "Of course, this is just a an initial look at sentiment analysis. You will see some more of this later in the module and in future modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132c9c5",
   "metadata": {},
   "source": [
    "Before that, we want to see what the sentiment analysis can tell us about engagement. Hence, we will analyze the average sentiment of tweets during raceday and add this to our `racingdf` (we first need to add a new column so that we can adjust the individual entries):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa24096",
   "metadata": {},
   "outputs": [],
   "source": [
    "racingdf['raceday_sentiment'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for racedate in racedates:\n",
    "    sentiment = 0\n",
    "    tweet_count = 0\n",
    "    for tweet in tweets:\n",
    "        if (pd.to_datetime(tweet.created_at.date()) - racedate).days == 0:\n",
    "            sentiment += sia.polarity_scores(tweet.text)['compound']\n",
    "            tweet_count += 1\n",
    "    racingdf.loc[racingdf['date'] == racedate,\"raceday_sentiment\"] = sentiment / tweet_count\n",
    "racingdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf1a5f",
   "metadata": {},
   "source": [
    "### Discussion point: Does this triangulate our previous findings? What about the saying \"all news is good news\"?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtvc_env",
   "language": "python",
   "name": "dtvc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
