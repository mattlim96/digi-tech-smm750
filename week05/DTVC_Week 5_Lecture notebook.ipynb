{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJ271Cky2c8L"
   },
   "source": [
    "# LATENT DIRICHLET ALLOCATION (LDA) TOPIC MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fitxntl4CXB4"
   },
   "source": [
    "In this notebook, we will see **how to run the LDA version of topic modeling** and **how to use the outputs** generated by the algorithm.  \n",
    "\n",
    "In more details, this code will help you  performing the following **steps** :\n",
    "*   Run a topic model on a large corpus of text\n",
    "*   Understand the structure of the outputs generated by LDA\n",
    "*   Interpret the topics generated by the algorithm\n",
    "\n",
    "We will perform the steps above on a corpus of text that comprises all the reviews that **Chimera** employees post about their company on the website **Glassdoor.com**. The employee reviews are structured in a way that allows users to separate the \"pros\" and \"cons\" of their organization when posting the content on Glassdoor. The analysis can be repeated for the \"pros\" and \"cons\" separately.\n",
    "\n",
    "Notice: the code is set up to run on the \"pros\" section of the Glassdoor reviews only. To perform the analysis on the \"cons\", you only need to adjust which of the Excel sheets gets loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cg6ro8Ss4fhx"
   },
   "source": [
    "## Step 0: Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNF3G0PKCcG7"
   },
   "source": [
    "**Modules import**\n",
    "\n",
    "Let's start by importing the modules needed to run the algorithm. Two libraries you probably haven't installed yet are `openpyxl` and `spacy`, so let's take care of that (note, `openpyxl` is not loaded here separately, but is used by pandas to open xlsx files). Within `spacy`, we will also use the `en_core_web_sm` library, which we need to download directly from Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nmS77kLCh-M",
    "outputId": "413d0e6c-b5e5-4eaf-889b-0ec0b0e3d861"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mattheus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Lh_d7vnDDC9"
   },
   "source": [
    "## Step 1: Upload the corpus of text to analyse with topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIo44TPYNRw_"
   },
   "source": [
    "**Import Glassdoor review data**\n",
    "\n",
    "Load Excel spreasheet storing the Chimera review data available on Glassdoor and inspect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "shNf2QHczH6M",
    "outputId": "bae6e84b-0cfc-4cf9-cd6f-4165185928bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review text-pros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You will be working with the most talented ppl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Freedom and responsibility. You're treated lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great colleagues -- incredible really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The upper management of Chimera really does se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The people there are fantastic, the service is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Review text-pros\n",
       "0  You will be working with the most talented ppl...\n",
       "1  Freedom and responsibility. You're treated lik...\n",
       "2              Great colleagues -- incredible really\n",
       "3  The upper management of Chimera really does se...\n",
       "4  The people there are fantastic, the service is..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# isolate \"pros\" section of reviews\n",
    "chimera_df = pd.read_excel(\"ReviewData.xlsx\", sheet_name = 'Pros')\n",
    "chimera_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gqutwio-OgmZ"
   },
   "source": [
    "**Extract review text to analyse**\n",
    "\n",
    "From `chimera_df`, isolate the reviews text to analyse with topic modeling and do some pre-cleaning on the strings of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q0_eB8NbDum7",
    "outputId": "4e169aac-652a-46fc-9505-1ce994203103"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you will be working with the most talented ppl around',\n",
       " \"freedom and responsibility you're treated like an adult you're part of a pro team that is highly functioning the company is well respected and has a super positive brand awareness i'm never able to go anywhere without getting peppered with raves and happy customer comments and i love wearing company logo gear when i'm out for this reason you matter at there is no dead wood everyone is doing something very important to the company or they wouldn't be there what you are doing matters and makes a difference and that feels good you can really make a difference no matter what your role the company often repeats that as an employee you're not on an olympic team meaning you don't do this / with no life with all focus on the gold we're more of a varsity team playing to win very good but still have to balance with classes and learning we need to have a good work/life balance and its very important to the company that we do i think the best thing and its subtle is this no vacation/no holiday stuff you work when you work they seem to realize that everyone is working hard and all the time nights weekends often a ton of hard work so if you want to take time off for a vacation a long holiday a day off whatever it happens at your own discretion you don't ask you don't get permission you just do what you have to do no one is keeping track i've never heard of this kind of policy and you cannot imagine what it doest for your morale for feeling like you're being treated like a grown up for that rule alone i think it stands beyond any other org\",\n",
       " 'great colleagues incredible really',\n",
       " \"the upper management of really does seem to want to take a different approach with how they handle their employees and accepts the fact that as an adult you're able to get your job done they appear to be very with it the call center attempts to echo that sentiment and provides many perks that make work an alright place to be from the free coffee and refreshments in the cafeteria to the free rental plan great benefits and competitive pay there is a great ideal that the company attempts to live up to that the people populating the center are worth the extra buck or two it was refreshing to work for a company that was capable of identifying problems and instead of sweeping them under the rug took the initiative to change the problem\",\n",
       " 'the people there are fantastic the service is great the facilities are terrific as well as comfortable some of the perks were worth it (free !) the training team is great the support staff are incredible all very hard working people the representatives on the floor are without a doubt the best in the industry and who doesn\\'t want to work \"in the movies\"? the pay is pretty decent the benefits are acceptable']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chimera_df = chimera_df.fillna('')\n",
    "data_pros = chimera_df['Review text-pros'].tolist()\n",
    "data_pros_cleaned = []\n",
    "for item in data_pros:\n",
    "    if item != \"\":\n",
    "        item_modified = item.lower().replace(\"chimera\", \" \") # the company name will not be meaningful to interpretation\n",
    "        item_modified = item_modified.replace(\"show less\", \"\") # Show less and show more buttons are left-over from the data entry froms\n",
    "        item_modified = item_modified.replace(\"show more\", \"\")\n",
    "        item_modified = item_modified.replace(\"\\n\", \" \") # \\n indicate line-breaks that we do not care about\n",
    "        item_modified = item_modified.replace(\"_x000d_\", \"\") # when reading data from .xlsx files, this will appear around line breaks\n",
    "        item_modified = item_modified.replace(\".\", \"\") # Remove periods\n",
    "        item_modified = item_modified.replace(\",\", \"\") # Remove commata\n",
    "        item_modified = item_modified.replace(\"-\", \" \") # Remove dashes\n",
    "        item_modified = re.sub(r\"\\s+\", \" \", item_modified) # Remove extra whitespace\n",
    "        item_modified =  ''.join([i for i in item_modified if not i.isdigit()]) # taking out numbers\n",
    "        data_pros_cleaned.append(item_modified)\n",
    "\n",
    "data_pros_cleaned[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gISoOGRv2x1s"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21kNeqgCMQ5v"
   },
   "source": [
    "### Step 1.1: More on data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWtN3_38RPhS"
   },
   "source": [
    "The next few steps will provide you an example of lemmatization (similar to stemming) and removal of stop words, both necessary cleaning steps to perform prior to running the topic model. \n",
    "\n",
    "As an example, we will use the spacy library English language model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hUMsHVMuNZkL"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') # we first load the English language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SvxMlEpRj0I"
   },
   "source": [
    "Let's use one review from the corpus as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onA7cO0GNoms",
    "outputId": "823f1a91-d6e0-44ed-cd96-0d4b0510a65c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benefits are terrific and i have my perfect shift free movies are always wonderful this is the perfect job for me because it really is low stress and it is a perfect avenue for my love of film film history and general film knowledge\n"
     ]
    }
   ],
   "source": [
    "example_review = data_pros_cleaned[5]\n",
    "parsed_review = nlp(example_review)\n",
    "print (parsed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bk6sU4Z5NZR0"
   },
   "source": [
    "We will next inspect each \"token\" in our sample review parsed_review, understand what lemmatizing means, and figure out whether each word should be kept in the review for the final analysis or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['benefits', 'are', 'terrific', 'and', 'i', 'have', 'my', 'perfect', 'shift', 'free', 'movies', 'are', 'always', 'wonderful', 'this', 'is', 'the', 'perfect', 'job', 'for', 'me', 'because', 'it', 'really', 'is', 'low', 'stress', 'and', 'it', 'is', 'a', 'perfect', 'avenue', 'for', 'my', 'love', 'of', 'film', 'film', 'history', 'and', 'general', 'film', 'knowledge']\n"
     ]
    }
   ],
   "source": [
    "# take out individual tokens from parsed review\n",
    "token_text = [token.text for token in parsed_review]\n",
    "print(token_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['benefit', 'be', 'terrific', 'and', 'I', 'have', 'my', 'perfect', 'shift', 'free', 'movie', 'be', 'always', 'wonderful', 'this', 'be', 'the', 'perfect', 'job', 'for', 'I', 'because', 'it', 'really', 'be', 'low', 'stress', 'and', 'it', 'be', 'a', 'perfect', 'avenue', 'for', 'my', 'love', 'of', 'film', 'film', 'history', 'and', 'general', 'film', 'knowledge']\n"
     ]
    }
   ],
   "source": [
    "# lemmatize words\n",
    "token_lemmas = [token.lemma_ for token in parsed_review]\n",
    "print(token_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 'benefit' is not a stopword\n",
      "Token 'be' is a stopword\n",
      "Token 'terrific' is not a stopword\n",
      "Token 'and' is a stopword\n",
      "Token 'I' is a stopword\n",
      "Token 'have' is a stopword\n",
      "Token 'my' is a stopword\n",
      "Token 'perfect' is not a stopword\n",
      "Token 'shift' is not a stopword\n",
      "Token 'free' is not a stopword\n",
      "Token 'movie' is not a stopword\n",
      "Token 'be' is a stopword\n",
      "Token 'always' is a stopword\n",
      "Token 'wonderful' is not a stopword\n",
      "Token 'this' is a stopword\n",
      "Token 'be' is a stopword\n",
      "Token 'the' is a stopword\n",
      "Token 'perfect' is not a stopword\n",
      "Token 'job' is not a stopword\n",
      "Token 'for' is a stopword\n",
      "Token 'I' is a stopword\n",
      "Token 'because' is a stopword\n",
      "Token 'it' is a stopword\n",
      "Token 'really' is a stopword\n",
      "Token 'be' is a stopword\n",
      "Token 'low' is not a stopword\n",
      "Token 'stress' is not a stopword\n",
      "Token 'and' is a stopword\n",
      "Token 'it' is a stopword\n",
      "Token 'be' is a stopword\n",
      "Token 'a' is a stopword\n",
      "Token 'perfect' is not a stopword\n",
      "Token 'avenue' is not a stopword\n",
      "Token 'for' is a stopword\n",
      "Token 'my' is a stopword\n",
      "Token 'love' is not a stopword\n",
      "Token 'of' is a stopword\n",
      "Token 'film' is not a stopword\n",
      "Token 'film' is not a stopword\n",
      "Token 'history' is not a stopword\n",
      "Token 'and' is a stopword\n",
      "Token 'general' is not a stopword\n",
      "Token 'film' is not a stopword\n",
      "Token 'knowledge' is not a stopword\n"
     ]
    }
   ],
   "source": [
    "# check if token is a stop word\n",
    "token_stop = [token.is_stop for token in parsed_review]\n",
    "for t in range(len(token_lemmas)):\n",
    "    if token_stop[t]:\n",
    "        print(\"Token '\" + token_lemmas[t] + \"' is a stopword\")\n",
    "    else:\n",
    "        print(\"Token '\" + token_lemmas[t] + \"' is not a stopword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tjr85l1FMOco",
    "outputId": "639c9dea-95b7-4b08-d777-da10d4a1f330"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Text</th>\n",
       "      <th>Stemmed Text</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>benefits</td>\n",
       "      <td>benefit</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>are</td>\n",
       "      <td>be</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>terrific</td>\n",
       "      <td>terrific</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i</td>\n",
       "      <td>I</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>have</td>\n",
       "      <td>have</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>my</td>\n",
       "      <td>my</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>perfect</td>\n",
       "      <td>perfect</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>shift</td>\n",
       "      <td>shift</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>free</td>\n",
       "      <td>free</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>movies</td>\n",
       "      <td>movie</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>are</td>\n",
       "      <td>be</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>always</td>\n",
       "      <td>always</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>perfect</td>\n",
       "      <td>perfect</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>job</td>\n",
       "      <td>job</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Original Text Stemmed Text  stopwords\n",
       "0       benefits      benefit      False\n",
       "1            are           be       True\n",
       "2       terrific     terrific      False\n",
       "3            and          and       True\n",
       "4              i            I       True\n",
       "5           have         have       True\n",
       "6             my           my       True\n",
       "7        perfect      perfect      False\n",
       "8          shift        shift      False\n",
       "9           free         free      False\n",
       "10        movies        movie      False\n",
       "11           are           be       True\n",
       "12        always       always       True\n",
       "13     wonderful    wonderful      False\n",
       "14          this         this       True\n",
       "15            is           be       True\n",
       "16           the          the       True\n",
       "17       perfect      perfect      False\n",
       "18           job          job      False\n",
       "19           for          for       True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assemble results for inspection\n",
    "pd.DataFrame(zip(token_text, token_lemmas, token_stop), columns=['Original Text', 'Stemmed Text', 'stopwords']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3Lb3KV_49mJ"
   },
   "source": [
    "### Step 1.2: The actual stemming process\n",
    "\n",
    "We actually stem the text in the corpus using a `nltk` based class, but we won't see very much of the process. Don't worry about the details here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0kxDDSRgC4uf"
   },
   "outputs": [],
   "source": [
    "snowball_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class SBStemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(SBStemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([snowball_stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mattheus\\anaconda3\\envs\\smm750\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = SBStemmedCountVectorizer(max_df = 0.9, min_df=0.01, analyzer=\"word\", stop_words='english') #max min required occurrence of word, english stop words\n",
    "tf = tf_vectorizer.fit_transform(data_pros_cleaned) # vectorize data (learn the vocabulary dictionary and return term-document matrix)\n",
    "voc = tf_vectorizer.get_feature_names() # extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8EiZn9DGg0K"
   },
   "source": [
    "## Step 2: Run LDA Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyper-parameter tuning** is an essential step in using any ML algorithm and it is one of the main steps prior to actually run the LDA model. \n",
    "\n",
    "We will not discuss how parameters are tuned here, but you can see some details further below in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jk45e0A7WgzF"
   },
   "source": [
    "**Select optimal topic number, K**\n",
    "\n",
    "Based on the result of a computation-heavy step that has been already run in background (coherence maximization algorithm to tune number of topics K), we will select the optimal number of topics to fit the topic model on the Glassdoor reviews available for Chimera.\n",
    "\n",
    "While we could be running the topic model with an arbitrary number of topics, tuning such a parameter is very important, as it (1) favours the accuracy of how data are represented as topics, and (2) increases the replicability of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Q9skI5ZCWpmD"
   },
   "outputs": [],
   "source": [
    "optimal_topics_pros = 157  # For cons: 207"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfSceCuqGzst"
   },
   "source": [
    "**Run LDA topic model on review text with optimal topic number**\n",
    "\n",
    "We will fit the LDA topic model on the \"pros\" and \"cons\" sections of the Chimera text reviews, separately. \n",
    "\n",
    "LDA takes as an input a matrix of documents X words (terms), generated through the vectorization stage. \n",
    "\n",
    "You can find more details about how to run LDA using the sklearn library (e.g., about the meaning of the various parameters in the function, and how to best tune those) [here](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Q_0uFnnpGqup"
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=optimal_topics_pros, max_iter=200, learning_method='batch', learning_offset=10.,evaluate_every=2,random_state=1234)\n",
    "lda_fit = lda.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9Vn_CVVkDvV"
   },
   "source": [
    "### Step 2.1: LDA output - Topic-over-word matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5it9sUIfkewy"
   },
   "source": [
    "We are finally able to generate one of the two main outputs of the topic model: a **matrix summarizing the distribution of the topics** estimated by LDA **over the words forming the corpus vocabulary** (i.e., all the unique words that appear in the documents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaWSC4slk2PT"
   },
   "source": [
    "Let's start by **visualizing the vocabulary**, how big it is, and the words it comprises: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIqrXFkzj9RS",
    "outputId": "12c0548c-a8aa-48fc-e3e9-7456722aa2f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print length of vocabulary list\n",
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKcdCb5pj_VD",
    "outputId": "ccd60851-7654-472d-bf6c-e7a4dac923d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abil',\n",
       " 'abl',\n",
       " 'account',\n",
       " 'actual',\n",
       " 'adult',\n",
       " 'allow',\n",
       " 'amaz',\n",
       " 'appreci',\n",
       " 'approv',\n",
       " 'area',\n",
       " 'ask',\n",
       " 'atmospher',\n",
       " 'autonomi',\n",
       " 'avail',\n",
       " 'averag',\n",
       " 'awesom',\n",
       " 'balanc',\n",
       " 'base',\n",
       " 'benefit',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'bonus',\n",
       " 'brand',\n",
       " 'break',\n",
       " 'bright',\n",
       " 'brilliant',\n",
       " 'bring',\n",
       " 'build',\n",
       " 'busi',\n",
       " 'cafeteria',\n",
       " 'capabl',\n",
       " 'care',\n",
       " 'career',\n",
       " 'cater',\n",
       " 'center',\n",
       " 'ceo',\n",
       " 'challeng',\n",
       " 'chang',\n",
       " 'cheap',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'code',\n",
       " 'coffe',\n",
       " 'collabor',\n",
       " 'colleagu',\n",
       " 'come',\n",
       " 'communic',\n",
       " 'compani',\n",
       " 'compens',\n",
       " 'compet',\n",
       " 'competit',\n",
       " 'complet',\n",
       " 'consid',\n",
       " 'consist',\n",
       " 'constant',\n",
       " 'continu',\n",
       " 'cool',\n",
       " 'corpor',\n",
       " 'cowork',\n",
       " 'creat',\n",
       " 'creativ',\n",
       " 'cultur',\n",
       " 'custom',\n",
       " 'cut',\n",
       " 'data',\n",
       " 'day',\n",
       " 'decent',\n",
       " 'decid',\n",
       " 'decis',\n",
       " 'deck',\n",
       " 'deliv',\n",
       " 'dental',\n",
       " 'depart',\n",
       " 'develop',\n",
       " 'did',\n",
       " 'differ',\n",
       " 'direct',\n",
       " 'do',\n",
       " 'doe',\n",
       " 'doesn',\n",
       " 'don',\n",
       " 'dress',\n",
       " 'drink',\n",
       " 'drive',\n",
       " 'driven',\n",
       " 'dvd',\n",
       " 'dynam',\n",
       " 'easi',\n",
       " 'edg',\n",
       " 'employ',\n",
       " 'employe',\n",
       " 'encourag',\n",
       " 'engag',\n",
       " 'engin',\n",
       " 'enjoy',\n",
       " 'entertain',\n",
       " 'environ',\n",
       " 'exact',\n",
       " 'excel',\n",
       " 'except',\n",
       " 'excit',\n",
       " 'execut',\n",
       " 'expect',\n",
       " 'experi',\n",
       " 'extra',\n",
       " 'extrem',\n",
       " 'facil',\n",
       " 'fair',\n",
       " 'fantast',\n",
       " 'far',\n",
       " 'fast',\n",
       " 'feedback',\n",
       " 'feel',\n",
       " 'fire',\n",
       " 'fit',\n",
       " 'flat',\n",
       " 'flexibl',\n",
       " 'focus',\n",
       " 'food',\n",
       " 'forward',\n",
       " 'free',\n",
       " 'freedom',\n",
       " 'friend',\n",
       " 'fun',\n",
       " 'game',\n",
       " 'general',\n",
       " 'get',\n",
       " 'give',\n",
       " 'given',\n",
       " 'goal',\n",
       " 'good',\n",
       " 'great',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'growth',\n",
       " 'hand',\n",
       " 'happi',\n",
       " 'hard',\n",
       " 'have',\n",
       " 'health',\n",
       " 'help',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'hire',\n",
       " 'honest',\n",
       " 'hour',\n",
       " 'hr',\n",
       " 'idea',\n",
       " 'impact',\n",
       " 'import',\n",
       " 'impress',\n",
       " 'improv',\n",
       " 'includ',\n",
       " 'incred',\n",
       " 'individu',\n",
       " 'industri',\n",
       " 'initi',\n",
       " 'innov',\n",
       " 'insur',\n",
       " 'intellig',\n",
       " 'interest',\n",
       " 'job',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'kind',\n",
       " 'know',\n",
       " 'knowledg',\n",
       " 'laid',\n",
       " 'lead',\n",
       " 'leader',\n",
       " 'leadership',\n",
       " 'learn',\n",
       " 'leav',\n",
       " 'let',\n",
       " 'level',\n",
       " 'life',\n",
       " 'like',\n",
       " 'littl',\n",
       " 'live',\n",
       " 'll',\n",
       " 'locat',\n",
       " 'long',\n",
       " 'look',\n",
       " 'lot',\n",
       " 'love',\n",
       " 'low',\n",
       " 'lunch',\n",
       " 'make',\n",
       " 'manag',\n",
       " 'market',\n",
       " 'match',\n",
       " 'matter',\n",
       " 'meal',\n",
       " 'mean',\n",
       " 'meet',\n",
       " 'micromanag',\n",
       " 'minim',\n",
       " 'money',\n",
       " 'month',\n",
       " 'motiv',\n",
       " 'move',\n",
       " 'movi',\n",
       " 'need',\n",
       " 'new',\n",
       " 'nice',\n",
       " 'number',\n",
       " 'offer',\n",
       " 'offic',\n",
       " 'open',\n",
       " 'opportun',\n",
       " 'option',\n",
       " 'organ',\n",
       " 'orient',\n",
       " 'overal',\n",
       " 'pace',\n",
       " 'packag',\n",
       " 'paid',\n",
       " 'passion',\n",
       " 'pay',\n",
       " 'peer',\n",
       " 'peopl',\n",
       " 'perform',\n",
       " 'perk',\n",
       " 'person',\n",
       " 'phone',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'plenti',\n",
       " 'plus',\n",
       " 'polici',\n",
       " 'polit',\n",
       " 'posit',\n",
       " 'possibl',\n",
       " 'practic',\n",
       " 'pretti',\n",
       " 'problem',\n",
       " 'process',\n",
       " 'product',\n",
       " 'profession',\n",
       " 'project',\n",
       " 'promot',\n",
       " 'pros',\n",
       " 'provid',\n",
       " 'pto',\n",
       " 'push',\n",
       " 'qualiti',\n",
       " 'quick',\n",
       " 'rais',\n",
       " 'rate',\n",
       " 'real',\n",
       " 'realli',\n",
       " 'reason',\n",
       " 'reed',\n",
       " 'relat',\n",
       " 'relax',\n",
       " 'respect',\n",
       " 'respons',\n",
       " 'result',\n",
       " 'resum',\n",
       " 'review',\n",
       " 'reward',\n",
       " 'right',\n",
       " 'risk',\n",
       " 'room',\n",
       " 'run',\n",
       " 'salari',\n",
       " 'say',\n",
       " 'schedul',\n",
       " 'self',\n",
       " 'senior',\n",
       " 'servic',\n",
       " 'set',\n",
       " 'share',\n",
       " 'shift',\n",
       " 'show',\n",
       " 'small',\n",
       " 'smart',\n",
       " 'snack',\n",
       " 'solid',\n",
       " 'solut',\n",
       " 'solv',\n",
       " 'stand',\n",
       " 'start',\n",
       " 'stay',\n",
       " 'stock',\n",
       " 'strategi',\n",
       " 'stream',\n",
       " 'strong',\n",
       " 'stuff',\n",
       " 'stun',\n",
       " 'subscript',\n",
       " 'success',\n",
       " 'super',\n",
       " 'supervisor',\n",
       " 'support',\n",
       " 'sure',\n",
       " 'surround',\n",
       " 'swag',\n",
       " 'take',\n",
       " 'talent',\n",
       " 'talk',\n",
       " 'team',\n",
       " 'tech',\n",
       " 'technic',\n",
       " 'technolog',\n",
       " 'tell',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'time',\n",
       " 'ton',\n",
       " 'tool',\n",
       " 'track',\n",
       " 'train',\n",
       " 'transpar',\n",
       " 'treat',\n",
       " 'tri',\n",
       " 'true',\n",
       " 'trust',\n",
       " 'understand',\n",
       " 'uniqu',\n",
       " 'unlimit',\n",
       " 'upper',\n",
       " 'use',\n",
       " 'vacat',\n",
       " 'valley',\n",
       " 'valu',\n",
       " 've',\n",
       " 'vision',\n",
       " 'wage',\n",
       " 'want',\n",
       " 'watch',\n",
       " 'way',\n",
       " 'week',\n",
       " 'will',\n",
       " 'wonder',\n",
       " 'work',\n",
       " 'worker',\n",
       " 'world',\n",
       " 'year']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print words forming the vocabulary\n",
    "voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b__ydoXlM2Q"
   },
   "source": [
    "Let's now generate the **matrix summarizing the distributions of topics over words** and inspect it. \n",
    "\n",
    "As we will see below, this matrix can be used to interpret the meaning of the topics estimated by LDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lMZ7pEPojZVf",
    "outputId": "8abe6f5d-64a0-40b4-c62b-636c3bb00334"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 340)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution of topics over words in vocabulary\n",
    "topics_words = lda_fit.components_ / lda_fit.components_.sum(axis=1)[:, np.newaxis]\n",
    "topics_words_df = pd.DataFrame(topics_words)\n",
    "topics_words_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "z_xjWTXkj2mm",
    "outputId": "bf06a64c-4f9b-42ae-c2a9-905649f5466e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>330</th>\n",
       "      <th>331</th>\n",
       "      <th>332</th>\n",
       "      <th>333</th>\n",
       "      <th>334</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.041499</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.013569</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013569</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.080986</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.156698</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.021558</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.021558</td>\n",
       "      <td>0.064401</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.021558</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.058415</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.024271</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.024271</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.024271</td>\n",
       "      <td>0.000154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 340 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.000263  0.000263  0.000263  0.000263  0.000263  0.000263  0.041499   \n",
       "1  0.000086  0.000086  0.000086  0.013569  0.000086  0.000086  0.000086   \n",
       "2  0.000045  0.000045  0.000045  0.000045  0.000045  0.000045  0.000045   \n",
       "3  0.021558  0.000136  0.000136  0.000136  0.021558  0.064401  0.000136   \n",
       "4  0.000154  0.000154  0.000154  0.000154  0.024271  0.000154  0.000154   \n",
       "\n",
       "        7         8         9    ...       330       331       332       333  \\\n",
       "0  0.000263  0.000263  0.000263  ...  0.000263  0.000263  0.000263  0.000263   \n",
       "1  0.000086  0.000086  0.000086  ...  0.013569  0.000086  0.000086  0.000086   \n",
       "2  0.000045  0.000045  0.000045  ...  0.000045  0.000045  0.000045  0.000045   \n",
       "3  0.000136  0.000136  0.000136  ...  0.000136  0.000136  0.021558  0.000136   \n",
       "4  0.000154  0.000154  0.000154  ...  0.000154  0.000154  0.000154  0.000154   \n",
       "\n",
       "        334       335       336       337       338       339  \n",
       "0  0.000263  0.000263  0.000263  0.000263  0.000263  0.000263  \n",
       "1  0.000086  0.000086  0.080986  0.000086  0.000086  0.000086  \n",
       "2  0.000045  0.000045  0.156698  0.000045  0.000045  0.000045  \n",
       "3  0.000136  0.000136  0.058415  0.000136  0.000136  0.000136  \n",
       "4  0.024271  0.000154  0.000154  0.000154  0.024271  0.000154  \n",
       "\n",
       "[5 rows x 340 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect matrix\n",
    "topics_words_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyW60aafG6ys"
   },
   "source": [
    "### Step 2.2: LDA output - Document-over-topic matrix\n",
    "\n",
    "The second output of interest, which we are going to generate and inspect below, is a **matrix summarizing the distribution of documents** (i.e., employee reviews, in our context) **over the topics** (i.e., lists of words that, statistically, co-occur across the reviews more likely than others) estimated by LDA. \n",
    "\n",
    "As we will see below, this matrix can be used to compute summary statistics and to interpret the topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "_PWcrzlqG_RU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(692, 157)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution of documents over topics\n",
    "docs_topics = lda.transform(tf)\n",
    "docs_topics_df = pd.DataFrame(data = docs_topics, index=None, columns=None, dtype=None, copy=False)\n",
    "docs_topics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "XlMz-fHIjvPX",
    "outputId": "1cef0122-3984-4420-9665-c7033d8c82bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.692553</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 157 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.002123  0.002123  0.002123  0.002123  0.002123  0.002123  0.002123   \n",
       "1  0.000075  0.000075  0.000075  0.000075  0.000075  0.000075  0.000075   \n",
       "2  0.001274  0.001274  0.001274  0.001274  0.001274  0.001274  0.001274   \n",
       "3  0.000163  0.000163  0.000163  0.000163  0.000163  0.000163  0.000163   \n",
       "4  0.000245  0.000245  0.000245  0.000245  0.000245  0.000245  0.000245   \n",
       "\n",
       "        7         8         9    ...       147       148       149       150  \\\n",
       "0  0.002123  0.002123  0.002123  ...  0.002123  0.002123  0.002123  0.002123   \n",
       "1  0.000075  0.000075  0.000075  ...  0.000075  0.000075  0.000075  0.000075   \n",
       "2  0.001274  0.001274  0.001274  ...  0.001274  0.001274  0.001274  0.001274   \n",
       "3  0.000163  0.000163  0.000163  ...  0.000163  0.000163  0.000163  0.000163   \n",
       "4  0.000245  0.000245  0.000245  ...  0.000245  0.000245  0.000245  0.000245   \n",
       "\n",
       "        151       152       153       154       155       156  \n",
       "0  0.002123  0.002123  0.002123  0.002123  0.002123  0.002123  \n",
       "1  0.000075  0.000075  0.000075  0.000075  0.000075  0.000075  \n",
       "2  0.001274  0.001274  0.001274  0.001274  0.001274  0.001274  \n",
       "3  0.000163  0.000163  0.000163  0.000163  0.000163  0.000163  \n",
       "4  0.692553  0.000245  0.000245  0.000245  0.000245  0.000245  \n",
       "\n",
       "[5 rows x 157 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect matrix\n",
    "docs_topics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HFVX69xZThR"
   },
   "source": [
    "## Step 3: Use topic model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLk6edTec8u0"
   },
   "source": [
    "Once the topic model has been computed, we need to better understand how to use its outputs. \n",
    "\n",
    "We will focus on **interpreting the topics** (step 3.1) to map them into meaningful theoretical constructs.\n",
    "Further below in the notebook, you can find more on leveraging the document-over-topic matrix to **compute summary statistics** about culture (step 3.2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1: Interpreting the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1VyH0hug4Hb"
   },
   "source": [
    "Topic interpretation can be done in two ways: \n",
    "\n",
    "1.   Leveraging the **topic-over-word matrix**, find a coherent meaning across the words forming a topic (traditional approach, but not very replicable)\n",
    "2.   Leveraging the **document-over-topic matrix**, identify **salient topics** and **prototypical documents** to interpret them (more novel approach, improves reliability and replicability)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-_vTne1hqsq"
   },
   "source": [
    "#### Step 3.1.1: Interpret topics beased on distribution of topics over vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBjFhbIdiWLm"
   },
   "source": [
    "This approach consists in identifying the most relevant words from the vocabulary associated with each topics and find some shared meaning among them to generate a label for the topic.\n",
    "\n",
    "As you remember, in our example we have a vocabulary comprising 341 words, which we have inspected before. Each topic can be summarized as a distribution over the vocabulary, as we have seen already. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4t6KLFCnoMQ"
   },
   "source": [
    "**Extract main words per topic**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnKrzu86nRVo"
   },
   "source": [
    "Leveraging the topic-over-word distribution matrix, we can **visualize the main words associated with each topic** (e.g., top 10 words scoring the highest probability to belong to a topic). \n",
    "\n",
    "These can be used to label the topic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPATzxxapjbk"
   },
   "source": [
    "Let's create a function to visualize the top words (number to be specified) associated to each topic: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "q8AIGSPbnt_-"
   },
   "outputs": [],
   "source": [
    "def show_topics(vectorizer, lda_model, n_words):\n",
    "    keywords = np.array(tf_vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lutugR0p6bT"
   },
   "source": [
    "We can now **inspect the main 5 words associated to each topic** extracted by LDA and try to identify some coherent meaning they may convey. This would help to **generate a label** for the topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4vrHcifoaUi",
    "outputId": "3f036dc0-3d9a-41a9-9a7f-512bd8ed9402"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mattheus\\anaconda3\\envs\\smm750\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array(['idea', 'possibl', 'best', 'time', 'autonomi', 'respons', 'lot',\n",
       "        'level', 'communic', 'game'], dtype='<U10'),\n",
       " array(['work', 'compani', 'know', 'hard', 'expect', 'peer', 'big',\n",
       "        'place', 'challeng', 'salari'], dtype='<U10'),\n",
       " array(['good', 'work', 'abil', 'plus', 'plenti', 'plan', 'place', 'phone',\n",
       "        'person', 'perk'], dtype='<U10'),\n",
       " array(['person', 'make', 'allow', 'work', 'high', 'busi', 'decis',\n",
       "        'perform', 'hire', 'place'], dtype='<U10'),\n",
       " array(['take', 'extrem', 'flat', 'manag', 'compani', 'clear', 'high',\n",
       "        'competit', 'peer', 'great'], dtype='<U10'),\n",
       " array(['work', 'employe', 'environ', 'time', 'nice', 'lot', 'peopl',\n",
       "        'best', 'high', 'need'], dtype='<U10'),\n",
       " array(['food', 'lot', 'pay', 'account', 'get', 'team', 'freedom', 'small',\n",
       "        'low', 'share'], dtype='<U10'),\n",
       " array(['compani', 'peopl', 'feel', 'let', 'work', 'break', 'fast', 'year',\n",
       "        'fair', 'review'], dtype='<U10'),\n",
       " array(['packag', 'pay', 'work', 'long', 'show', 'great', 'resum', 'free',\n",
       "        'cultur', 'employe'], dtype='<U10'),\n",
       " array(['tell', 'compani', 'work', 'high', 'direct', 'impact', 'perform',\n",
       "        'busi', 'smart', 'environ'], dtype='<U10')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate topic-keyword association, to inspect\n",
    "topic_keywords = show_topics(tf_vectorizer, lda, n_words=10)\n",
    "topic_keywords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6avb-byHiPCD"
   },
   "source": [
    "#### Step 3.1.2: Interpret topics beased on salient topics and prototypical text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mqrXPFGqIWj"
   },
   "source": [
    "Although the approach proposed above to interpret topics based on they words they are mostly associated with is very common, it is quite subjective and not very replicable. \n",
    "\n",
    "Below, we will use an alternative method, which leverages the **distribution of documents over topics**, to interpret the topics algorithmically tuned by LDA. \n",
    "\n",
    "This approach consists in **identifying the most salient topics** in the corpus (i.e., the main topics many people are talking about), which one should interpret, and the **prototypical documents** associated with them, to be used to perform the topic interpretation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oroqr7swZuoH"
   },
   "source": [
    "**Identify most salient topics for interpretation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UgIg2BjIvbl"
   },
   "source": [
    "Those are the key topics in the corpus, which we should interpet. More formally, **salient topics** are those that have a large fraction of reviews from the corpus focused on those.\n",
    "\n",
    "The saliency parameter can be tuned, and sets the proportion of documents the topic must be feature in, beyond what can be expected by chance (1/K).\n",
    "\n",
    "Enter saliency parameter of your choice (must fall in [0,1] interval):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CE_L1I-rPcdX",
    "outputId": "f7c42459-9e9c-4b24-cd2a-6862f7427adb"
   },
   "outputs": [],
   "source": [
    "saliency_parameter = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVhUBLvGyFmW"
   },
   "source": [
    "The next block of code will tell us how many topics we'd need to interpret, as salient, based on our choice of `saliency_parameter` (again, don't worry too much about the details here!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rM8_3lonHG4t",
    "outputId": "4bbcf120-9bf4-4464-e195-89dfa855f74f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When considering the top 50.0 % of the distribution of documents over topics, the algorithm identifies 11.56% of total topics as most salient (equal to 18 topics).\n"
     ]
    }
   ],
   "source": [
    "docs_topics_masked = docs_topics_df.copy(deep='True')\n",
    "column_names = list(docs_topics_masked)\n",
    "\n",
    "for i in range(0, len(column_names)):\n",
    "    docs_topics_masked.loc[docs_topics_masked[i] > 1/optimal_topics_pros, i] = 1\n",
    "\n",
    "for i in range(0, len(column_names)):\n",
    "    docs_topics_masked.loc[docs_topics_masked[i] < 1, i] = 0\n",
    "\n",
    "topic_salience = docs_topics_masked.sum(axis=0)\n",
    "topic_salience_df = pd.DataFrame(topic_salience, columns= [\"reviews_pastThreshold\"])\n",
    "topic_salience_df = topic_salience_df.reset_index()\n",
    "topic_salience_df = topic_salience_df.rename(columns = {'index': 'topic_id'})\n",
    "topic_salience_df[\"percReviews\"] = topic_salience_df[\"reviews_pastThreshold\"] / docs_topics_df.shape[1]\n",
    "q = topic_salience_df[\"percReviews\"].quantile(saliency_parameter)\n",
    "topic_salience_df = topic_salience_df[topic_salience_df.percReviews >= q]\n",
    "\n",
    "salient_topics_perc = len(topic_salience_df)/ len(docs_topics_df)\n",
    "\n",
    "print (\"When considering the top \"+str(round((1-saliency_parameter)*100,2))+\" % of the distribution of documents over topics, the algorithm identifies \" \n",
    "       +str(round(salient_topics_perc *100,2))+\"% of total topics as most salient (equal to \" \n",
    "       +str(int(salient_topics_perc * len(list(docs_topics_df))))+\" topics).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ht9360GZH5Dx"
   },
   "source": [
    "**Extraction of prototypical reviews for interpretation and export**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwCeSClvUMJA"
   },
   "source": [
    "We need to choose a parameter *l* that a represents the minimal focus of a review on a topic for the review to be considered **prototypical for the topic** and thus included in the final topic interpretation.\n",
    "\n",
    "More on how *l* can be chosen can be found below in the additional parts.\n",
    "\n",
    "The code here finds the prototypical reviews for the different topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Ee8nT_G9ARPY"
   },
   "outputs": [],
   "source": [
    "l = 0.5  # For cons, a value of 0.4-0.5 seems to work fine\n",
    "\n",
    "rep_docs = [] #list saving, for each topic, the index of those reviews which load more than l on the topic\n",
    "for column in docs_topics_df: \n",
    "    rep_docs.append(docs_topics_df[(docs_topics_df[column]>l)==True].index.to_numpy())\n",
    "    \n",
    "row_topic = []\n",
    "i = 0\n",
    "for row in rep_docs: \n",
    "    topic_id = i\n",
    "    if len(rep_docs[topic_id]) == 0:\n",
    "        row_topic.append([topic_id])\n",
    "    else: \n",
    "        for j in range (0,len(rep_docs[topic_id])):\n",
    "            review_text = data_pros_cleaned[rep_docs[topic_id][j]]\n",
    "            review_id = rep_docs[topic_id][j]\n",
    "            row_topic.append([topic_id, review_text, review_id])\n",
    "    i = i+1\n",
    "    \n",
    "#convert into df\n",
    "row_topic_df = pd.DataFrame(row_topic, columns=[\"topic_id\", \"review_text\", \"review_id\"])\n",
    "row_topic_df = row_topic_df.dropna(axis=0)\n",
    "\n",
    "#extract prototypical reviews only for most salient topics\n",
    "prototipycalDocs_salientTopics_df = row_topic_df.merge(topic_salience_df, how = 'inner', left_on = 'topic_id', right_on = 'topic_id')\n",
    "\n",
    "#extract only relevant column_names\n",
    "prototipycalDocs_salientTopics_df = prototipycalDocs_salientTopics_df[['topic_id', 'review_text', 'review_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "al7F6cdfRNEf",
    "outputId": "2cac672d-e655-4861-d9e1-c165caa4d330"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>low bs small agile teams a lot gets done less ...</td>\n",
       "      <td>319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>pay seemingly lax supervisors food karts</td>\n",
       "      <td>336.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>pay is great but save your money the severance...</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>amazing culture not micro managing awesome emp...</td>\n",
       "      <td>437.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>decent hourly pay use of the eight out at a ti...</td>\n",
       "      <td>239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>an offer you cant resist</td>\n",
       "      <td>486.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>promotes camaraderie among employees with reas...</td>\n",
       "      <td>635.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>progressive work environment great internal wo...</td>\n",
       "      <td>346.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>great work environment great culture ideals</td>\n",
       "      <td>401.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16</td>\n",
       "      <td>environment is great love it</td>\n",
       "      <td>552.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>informal work environment diverse co workers</td>\n",
       "      <td>688.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>well paying for a call center free fruit cheap...</td>\n",
       "      <td>285.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17</td>\n",
       "      <td>the people are great free nice benefits</td>\n",
       "      <td>397.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>17</td>\n",
       "      <td>a very nice building in beverly hills</td>\n",
       "      <td>506.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21</td>\n",
       "      <td>smart group of (mostly) young people top of ma...</td>\n",
       "      <td>390.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>freedom and responsibility from the culture de...</td>\n",
       "      <td>359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>23</td>\n",
       "      <td>talented co workers unlimited vacation lots of...</td>\n",
       "      <td>553.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>24</td>\n",
       "      <td>the only reason to even consider this position...</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24</td>\n",
       "      <td>provide fairly well stocked break room and oft...</td>\n",
       "      <td>364.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>26</td>\n",
       "      <td>great co workers good support great pay</td>\n",
       "      <td>383.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_id                                        review_text  review_id\n",
       "0          6  low bs small agile teams a lot gets done less ...      319.0\n",
       "1          6           pay seemingly lax supervisors food karts      336.0\n",
       "2          8  pay is great but save your money the severance...       93.0\n",
       "3          8  amazing culture not micro managing awesome emp...      437.0\n",
       "4         13  decent hourly pay use of the eight out at a ti...      239.0\n",
       "5         13                           an offer you cant resist      486.0\n",
       "6         15  promotes camaraderie among employees with reas...      635.0\n",
       "7         16  progressive work environment great internal wo...      346.0\n",
       "8         16        great work environment great culture ideals      401.0\n",
       "9         16                       environment is great love it      552.0\n",
       "10        16       informal work environment diverse co workers      688.0\n",
       "11        17  well paying for a call center free fruit cheap...      285.0\n",
       "12        17            the people are great free nice benefits      397.0\n",
       "13        17              a very nice building in beverly hills      506.0\n",
       "14        21  smart group of (mostly) young people top of ma...      390.0\n",
       "15        23  freedom and responsibility from the culture de...      359.0\n",
       "16        23  talented co workers unlimited vacation lots of...      553.0\n",
       "17        24  the only reason to even consider this position...       60.0\n",
       "18        24  provide fairly well stocked break room and oft...      364.0\n",
       "19        26            great co workers good support great pay      383.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspection of prototypical documents associated to most salient topics\n",
    "prototipycalDocs_salientTopics_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYV6DODS0x_g"
   },
   "source": [
    "Export results to excel and download for inspection: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "mkf2ciVb0Xxm"
   },
   "outputs": [],
   "source": [
    "prototipycalDocs_salientTopics_df.to_excel(\"Chimera_TopicsToInterpret_Pros.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Additional steps in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before step 2: hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1riwo_USm2z"
   },
   "source": [
    "**Hyper-parameter tuning** is an essential step in using any ML algorithm and it is one of the main steps prior to actually run the LDA model. \n",
    "\n",
    "We will focus our attention on hyperparameters **alpha, i.e., the topic smoothing parameter**, and **K, i.e., the otpimal number of topics to run LDA**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxY8m8GqTPNh"
   },
   "source": [
    "**Understand the role of topic smoothing parameter, alpha**\n",
    "\n",
    "Alpha changes the \"spikiness\" of the document-over-topic distributions. We can see this through a simulation, in the following block of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6Ir4865VM_z"
   },
   "source": [
    "We create one new function for the purpose of the simulation. This simulates the structure of a topic model output (document-over-topic distribution matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "lQFcXMBeUkkd"
   },
   "outputs": [],
   "source": [
    "def create_alpha_vector(A, beta):\n",
    "    alpha_vector = beta*np.ones(A, dtype=np.int)\n",
    "    return alpha_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBK2YV-7VI0a"
   },
   "source": [
    "Let's set the parameters for the simulation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LT8yoAWHSmYh",
    "outputId": "676ab466-b52d-4f54-d9c4-3f30f62970af"
   },
   "outputs": [],
   "source": [
    "# number of topics (columns)\n",
    "K = 5\n",
    "# number of documents (rows)\n",
    "D = 3\n",
    "# alpha\n",
    "alpha = 0.9  # You can vary this between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxC1h8o4VvIi"
   },
   "source": [
    "And now we can simulate the topic model output to understand how alpha affects the distributions of documents over topics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "bx9SVowCVzfu",
    "outputId": "6c822634-7f5a-4891-9e37-e961fc6d42ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-c75f0a83561e>:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  alpha_vector = beta*np.ones(A, dtype=np.int)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.332576</td>\n",
       "      <td>0.303403</td>\n",
       "      <td>0.321568</td>\n",
       "      <td>0.021315</td>\n",
       "      <td>0.021137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.199781</td>\n",
       "      <td>0.015141</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.593431</td>\n",
       "      <td>0.097630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.043170</td>\n",
       "      <td>0.401795</td>\n",
       "      <td>0.188796</td>\n",
       "      <td>0.093716</td>\n",
       "      <td>0.272522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "0  0.332576  0.303403  0.321568  0.021315  0.021137\n",
       "1  0.199781  0.015141  0.094017  0.593431  0.097630\n",
       "2  0.043170  0.401795  0.188796  0.093716  0.272522"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simulate distributions of documents over topics\n",
    "docs_over_topics = np.random.dirichlet(create_alpha_vector(K, alpha), D)\n",
    "docs_over_topics_df = pd.DataFrame(docs_over_topics)\n",
    "docs_over_topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d4YJBuKUE6M"
   },
   "source": [
    "\n",
    "Usually, LDA is run with the default value alpha = 1/K, as we will also do in the example below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kv3daAuodSFa"
   },
   "source": [
    "**Understand the relationship between number of topics and topical coherence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpDrsM1ISNIb"
   },
   "source": [
    "Import modules needed for coherence computation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "vV3CD_JeOw1H"
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jx_NxQ3pfa5E"
   },
   "source": [
    "Create functions needed for coherence computation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "aJ3G8aKDffI1"
   },
   "outputs": [],
   "source": [
    "# used inside the coherence function\n",
    "def jsd(p, q, base=np.e):\n",
    "    p = np.asarray(p)\n",
    "    q = np.asarray(q)\n",
    "    m = 1./2*(p + q)\n",
    "    return sp.stats.entropy(p,m, base=base)/2. +  sp.stats.entropy(q, m, base=base)/2.\n",
    "\n",
    "def coherence(probMatrix):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(0, len(probMatrix)): \n",
    "        jsd_list = []\n",
    "        for j in range(0, len(probMatrix)): \n",
    "            jsd_list.append(jsd(probMatrix[i], probMatrix[j]))\n",
    "            j = j+1\n",
    "        df[str(i)] = jsd_list\n",
    "    mask = np.ones(df.shape,dtype='bool')\n",
    "    mask[np.triu_indices(len(df))] = False\n",
    "    df_lower_diagonal = df[(df>-1)&mask]\n",
    "   \n",
    "    distance_list = []\n",
    "    i = 0 \n",
    "    for i in range(0, len(df)): \n",
    "        column_list = df_lower_diagonal[str(i)].values.tolist() \n",
    "        column_lower_diagonal_list = [x for x in column_list if (math.isnan(x) == False)]\n",
    "        for d in column_lower_diagonal_list: \n",
    "            distance_list.append(d)\n",
    "        i = i + 1\n",
    "    coherence = sum(distance_list) / float(len(distance_list))\n",
    "    return coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JcVGNBFeSP2"
   },
   "source": [
    "Enter topic number for which you want to assess the solution coherence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8-HJioCdRxG",
    "outputId": "380fdfaf-b45b-4c95-8a98-679803c74116"
   },
   "outputs": [],
   "source": [
    "topics_number = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run LDA with chosen numer of topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=topics_number, max_iter=200, learning_method='batch', learning_offset=10.,evaluate_every=2,random_state=1234)\n",
    "lda_fit = lda.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m08qYRR5ebVN"
   },
   "source": [
    "Compute and print average topical coherence associated with the chosen number of topics (I have pre-computed the comparison point, as it takes quite some time to run the LDA with all the relevant values of `K`): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0lYNZto5ejXP",
    "outputId": "1de63242-8a64-46a4-9486-bcbeb7a7644e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you run LDA with 3 topics, the average topic coherence in the solution is equal to 0.36.\n",
      "Remember to benchmark this with the optimal coherence, equal to 0.58, obtained when running LDA with 157 topics.\n"
     ]
    }
   ],
   "source": [
    "topicsOverWords = lda_fit.components_ / lda_fit.components_.sum(axis=1)[:, np.newaxis]\n",
    "coherence_avg = coherence(topicsOverWords)\n",
    "\n",
    "if topics_number == 157:\n",
    "    print (\"You are at the optimal coherence solution: the average topical coherence is equal to 0.58.\")\n",
    "\n",
    "else: \n",
    "    print (\"When you run LDA with \"+str(topics_number)+\" topics, the average topic coherence in the solution is equal to \"+str(round(coherence_avg,2))+\".\\nRemember to benchmark this with the optimal coherence, equal to 0.58, obtained when running LDA with 157 topics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of step 3.1.2: Selection of parameter *l* to extract prototypical reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeuxKd6c5fub"
   },
   "source": [
    "The **parameter l** represents the minimal focus of a review on a topic for the review to be considered **prototypical for the topic** and thus included in the final topic interpretation.\n",
    "\n",
    "Choose the parameter *l* to maximize the percentage of interpretable topics in the solution, keeping in mind that you want to have a reasonable (i.e., not too high) number of reviews to screen, to label the topic content. The graph generated below as a function of *l* will help you making that choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "a0DzgerKHXvZ"
   },
   "outputs": [],
   "source": [
    "l_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "output_final = np.empty((0,6))\n",
    "\n",
    "for l in l_list: \n",
    "    rep_docs = [] #list saving, for each topic, the index of those reviews which load more than l on the topic\n",
    "    for column in docs_topics_df: \n",
    "        rep_docs.append(docs_topics_df[(docs_topics_df[column]>l)==True].index.to_numpy())\n",
    "     \n",
    "    row_topic = []\n",
    "    i = 0\n",
    "    for row in rep_docs: \n",
    "        topic_id = i\n",
    "        if len(rep_docs[topic_id]) == 0:\n",
    "            row_topic.append([topic_id])\n",
    "        else: \n",
    "            for j in range (0,len(rep_docs[topic_id])):\n",
    "                review_text = data_pros_cleaned[rep_docs[topic_id][j]]\n",
    "                review_id = rep_docs[topic_id][j]\n",
    "                row_topic.append([topic_id, review_text, review_id])\n",
    "        i = i+1\n",
    "    \n",
    "    row_topic_df = pd.DataFrame(row_topic, columns=[\"topic_id\", \"review_text\", \"review_id\"])   \n",
    "    \n",
    "    #generate summary stats for nr of prototypical reviews associated to topics\n",
    "    avg = row_topic_df.groupby(['topic_id']).topic_id.agg('count').describe()[1]\n",
    "    stdDev = row_topic_df.groupby(['topic_id']).topic_id.agg('count').describe()[2]\n",
    "    minVal = row_topic_df.groupby(['topic_id']).topic_id.agg('count').describe()[3]\n",
    "    maxVal = row_topic_df.groupby(['topic_id']).topic_id.agg('count').describe()[7]\n",
    "    \n",
    "    #compute M_k, i.e. number of documents extracted for each topic k for interpretation \n",
    "    #(to form prototypical text)\n",
    "    m_k = row_topic_df.groupby(['topic_id']).topic_id.agg('count').to_frame('M_k').reset_index()\n",
    "     \n",
    "    t = 1/l #threshold for topics to be interpreted (they need to have M_k>=t)\n",
    "    #keep only interpretable topics, given the specific value of l\n",
    "    m_k = m_k.drop(m_k[m_k.M_k < t].index)\n",
    "    \n",
    "    #compute percentage of interpretable topics\n",
    "    perc_int = len(m_k)/optimal_topics_pros\n",
    "\n",
    "    #save results\n",
    "\n",
    "    output=np.zeros((1,6))\n",
    "\n",
    "    #store results per firm   \n",
    "    output[0,0] = l\n",
    "    output[0,1] = perc_int\n",
    "    output[0,2] = avg\n",
    "    output[0,3] = stdDev\n",
    "    output[0,4] = minVal\n",
    "    output[0,5] = maxVal\n",
    "    \n",
    "    output_final = np.append(output_final, output, axis = 0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFvMPxC9yRQv"
   },
   "source": [
    "The next block of code will plot the percentage of interpretable topics as a function of *l*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "ROoY-W_EQIrd",
    "outputId": "5d42ea42-8cc4-40f7-ef84-b50ab470679a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_df = pd.DataFrame(output_final, columns = ['l', 'interpretableTopics', 'avgRev', 'stDevRev', 'minRev', 'maxRev'])\n",
    "output_df.plot.line(x='l', y='interpretableTopics', color='grey')\n",
    "\n",
    "plt.xlabel('l Threshold')\n",
    "plt.ylabel('% of interpretable topics')\n",
    "plt.title('Interpretable topics as function of l')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wz68k15GybpM"
   },
   "source": [
    "The next block of code will plot relevant statistics about the numnber of reviews associated with each salient topic as a funcion of *l*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "GjIyl8AiQIUd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_df.plot.line(x='l', y=['avgRev', 'stDevRev', 'minRev', 'maxRev'])\n",
    "\n",
    "plt.xlabel('l Threshold')\n",
    "plt.ylabel('Prototypical reviews')\n",
    "plt.title('Summary statistics: Prototypical reviews per topic')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set optimal l based on results graphed above**\n",
    "\n",
    "Your objective is to (1) maximize the percentage of topics from the corpus you can interpret, while (2) not having too many reviews to analyse for each topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_YRLxwXZhG1"
   },
   "source": [
    "### Step 3.2: Compute culture metrics on document-over-topic matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twQ6-SMkfgRx"
   },
   "source": [
    "The document-over-topic matrix can be used to compute distance metrics across the documents in the corpus, and to understand the breadth of content covered by each document (as in the Herfindahl–Hirschman Index).\n",
    "\n",
    "We compute three metrics of interest: average similarity, average focus, and average cross-entropy of the document-over-topic probability matrix. \n",
    "\n",
    "These three metrics summarize the shape of the matrix and tell us whether **people are similar in the content they write about** in their review and whether **they write many or few topics** in their document. \n",
    "\n",
    "Keep in mind that these metrics are not very useful, in absolute terms, and are generally used to compare corpora (summarizing, for instance, different organizations, or groups of individuals) among each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akqo2p9Qfxvf"
   },
   "source": [
    "Let's first define functions to compute the metrics, taking the document-over-topic probability matrix as input: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "_nFyAwGBb6Ip"
   },
   "outputs": [],
   "source": [
    "def similarity_func(probMatrix): #takes in input array \n",
    "    #Transform probMatrix_df into 2D array\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for i in range(0, len(probMatrix)): \n",
    "        jsd_list = []\n",
    "        for j in range(0, len(probMatrix)): \n",
    "            jsd_list.append(jsd(probMatrix[i], probMatrix[j]))\n",
    "            j = j+1\n",
    "        df[str(i)] = jsd_list\n",
    "\n",
    "    mask = np.ones(df.shape,dtype='bool')\n",
    "    mask[np.triu_indices(len(df))] = False\n",
    "    df_lower_diagonal = df[(df>-1)&mask]\n",
    "    \n",
    "    distance_list = []\n",
    "    i = 0 \n",
    "    for i in range(0, len(df)): \n",
    "    #Transform each column of df_lower_diagonal into list\n",
    "        column_list = df_lower_diagonal[str(i)].values.tolist()\n",
    "        #Drop nan values from column_list - to retain only actual values from lower diagonal \n",
    "        column_lower_diagonal_list = [x for x in column_list if (math.isnan(x) == False)]\n",
    "        for d in column_lower_diagonal_list: \n",
    "            distance_list.append(d)\n",
    "        i = i + 1\n",
    "    sim = 1-(sum(distance_list) / float(len(distance_list))) #this will return similarity, instead of comph\n",
    "    return sim\n",
    "\n",
    "def focus_func(probMatrix_df):  #takes in input pandas df \n",
    "    N = probMatrix_df.shape[0]\n",
    "    probMatrix = probMatrix_df.values\n",
    "    foc = (sum(map(sum, np.square(probMatrix))))/N\n",
    "    return foc \n",
    "\n",
    "def ent_avg(probMatrix):\n",
    "    entropy_list = []\n",
    "    for i in range(len(probMatrix)): \n",
    "        entropy_list.append(sp.stats.entropy(probMatrix[i]))\n",
    "    return np.mean(entropy_list)\n",
    "\n",
    "def cross_entropy(p, q):\n",
    "    for i in range(len(p)):\n",
    "        p[i] = p[i]+1e-12\n",
    "    for i in range(len(q)):\n",
    "        q[i] = q[i]+1e-12\n",
    "\n",
    "    return -sum([p[i] * np.log2(q[i]) for i in range(len(p))])\n",
    "\n",
    "def avg_crossEnt_func(probMatrix): #takes in input array \n",
    "\n",
    "    crossEntropy_list = []\n",
    "    for i in range(len(probMatrix)):\n",
    "        for j in range(len(probMatrix)): \n",
    "            if i > j:\n",
    "                crossEntropy_local_list = []\n",
    "                crossEntropy_local_list.append(cross_entropy(probMatrix[i], probMatrix[j]))\n",
    "                crossEntropy_local_list.append(cross_entropy(probMatrix[j], probMatrix[i]))\n",
    "                crossEntropy_list.append(np.mean(crossEntropy_local_list))\n",
    "        \n",
    "    return np.mean(crossEntropy_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the functions may take some time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-50-c76b5f7ed415>:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[str(i)] = jsd_list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41237132001592025\n",
      "0.3995251365905126\n",
      "10.368187923551263\n"
     ]
    }
   ],
   "source": [
    "similarity_score = similarity_func(docs_topics)\n",
    "focus_score = focus_func(docs_topics_df)\n",
    "ace_score = avg_crossEnt_func(docs_topics)\n",
    "print(similarity_score)\n",
    "print(focus_score)\n",
    "print(ace_score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TopicModeling_PhDWorkshop_2021",
   "provenance": []
  },
  "interpreter": {
   "hash": "df4e212b3ca48d28190f7fa319070b735bf716a22d4b5700005d4436fb499095"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('smm750': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
