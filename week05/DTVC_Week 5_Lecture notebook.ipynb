{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJ271Cky2c8L"
   },
   "source": [
    "# LATENT DIRICHLET ALLOCATION (LDA) TOPIC MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fitxntl4CXB4"
   },
   "source": [
    "In this notebook, we will see **how to run the LDA version of topic modeling** and **how to use the outputs** generated by the algorithm.  \n",
    "\n",
    "In more details, this code will help you  performing the following **steps** :\n",
    "*   Run a topic model on a large corpus of text\n",
    "*   Understand the structure of the outputs generated by LDA\n",
    "*   Interpret the topics generated by the algorithm\n",
    "\n",
    "We will perform the steps above on a corpus of text that comprises all the reviews that **Chimera** employees post about their company on the website **Glassdoor.com**. The employee reviews are structured in a way that allows users to separate the \"pros\" and \"cons\" of their organization when posting the content on Glassdoor. The analysis can be repeated for the \"pros\" and \"cons\" separately.\n",
    "\n",
    "Notice: the code is set up to run on the \"pros\" section of the Glassdoor reviews only. To perform the analysis on the \"cons\", you only need to adjust which of the Excel sheets gets loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cg6ro8Ss4fhx"
   },
   "source": [
    "## Step 0: Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNF3G0PKCcG7"
   },
   "source": [
    "**Modules import**\n",
    "\n",
    "Let's start by importing the modules needed to run the algorithm. Two libraries you probably haven't installed yet are `openpyxl` and `spacy`, so let's take care of that (note, `openpyxl` is not loaded here separately, but is used by pandas to open xlsx files). Within `spacy`, we will also use the `en_core_web_sm` library, which we need to download directly from Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nmS77kLCh-M",
    "outputId": "413d0e6c-b5e5-4eaf-889b-0ec0b0e3d861"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Lh_d7vnDDC9"
   },
   "source": [
    "## Step 1: Upload the corpus of text to analyse with topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIo44TPYNRw_"
   },
   "source": [
    "**Import Glassdoor review data**\n",
    "\n",
    "Load Excel spreasheet storing the Chimera review data available on Glassdoor and inspect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "shNf2QHczH6M",
    "outputId": "bae6e84b-0cfc-4cf9-cd6f-4165185928bf"
   },
   "outputs": [],
   "source": [
    "# isolate \"pros\" section of reviews\n",
    "chimera_df = pd.read_excel(\"ReviewData.xlsx\", sheet_name = 'Pros')\n",
    "chimera_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gqutwio-OgmZ"
   },
   "source": [
    "**Extract review text to analyse**\n",
    "\n",
    "From `chimera_df`, isolate the reviews text to analyse with topic modeling and do some pre-cleaning on the strings of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q0_eB8NbDum7",
    "outputId": "4e169aac-652a-46fc-9505-1ce994203103"
   },
   "outputs": [],
   "source": [
    "chimera_df = chimera_df.fillna('')\n",
    "data_pros = chimera_df['Review text-pros'].tolist()\n",
    "data_pros_cleaned = []\n",
    "for item in data_pros:\n",
    "    if item != \"\":\n",
    "        item_modified = item.lower().replace(\"chimera\", \" \") # the company name will not be meaningful to interpretation\n",
    "        item_modified = item_modified.replace(\"show less\", \"\") # Show less and show more buttons are left-over from the data entry froms\n",
    "        item_modified = item_modified.replace(\"show more\", \"\")\n",
    "        item_modified = item_modified.replace(\"\\n\", \" \") # \\n indicate line-breaks that we do not care about\n",
    "        item_modified = item_modified.replace(\"_x000d_\", \"\") # when reading data from .xlsx files, this will appear around line breaks\n",
    "        item_modified = item_modified.replace(\".\", \"\") # Remove periods\n",
    "        item_modified = item_modified.replace(\",\", \"\") # Remove commata\n",
    "        item_modified = item_modified.replace(\"-\", \" \") # Remove dashes\n",
    "        item_modified = re.sub(r\"\\s+\", \" \", item_modified) # Remove extra whitespace\n",
    "        item_modified =  ''.join([i for i in item_modified if not i.isdigit()]) # taking out numbers\n",
    "        data_pros_cleaned.append(item_modified)\n",
    "\n",
    "data_pros_cleaned[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gISoOGRv2x1s"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21kNeqgCMQ5v"
   },
   "source": [
    "### Step 1.1: More on data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWtN3_38RPhS"
   },
   "source": [
    "The next few steps will provide you an example of lemmatization (similar to stemming) and removal of stop words, both necessary cleaning steps to perform prior to running the topic model. \n",
    "\n",
    "As an example, we will use the spacy library English language model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUMsHVMuNZkL"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') # we first load the English language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SvxMlEpRj0I"
   },
   "source": [
    "Let's use one review from the corpus as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onA7cO0GNoms",
    "outputId": "823f1a91-d6e0-44ed-cd96-0d4b0510a65c"
   },
   "outputs": [],
   "source": [
    "example_review = data_pros_cleaned[5]\n",
    "parsed_review = nlp(example_review)\n",
    "print (parsed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bk6sU4Z5NZR0"
   },
   "source": [
    "We will next inspect each \"token\" in our sample review parsed_review, understand what lemmatizing means, and figure out whether each word should be kept in the review for the final analysis or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take out individual tokens from parsed review\n",
    "token_text = [token.text for token in parsed_review]\n",
    "print(token_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize words\n",
    "token_lemmas = [token.lemma_ for token in parsed_review]\n",
    "print(token_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if token is a stop word\n",
    "token_stop = [token.is_stop for token in parsed_review]\n",
    "for t in range(len(token_lemmas)):\n",
    "    if token_stop[t]:\n",
    "        print(\"Token '\" + token_lemmas[t] + \"' is a stopword\")\n",
    "    else:\n",
    "        print(\"Token '\" + token_lemmas[t] + \"' is not a stopword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tjr85l1FMOco",
    "outputId": "639c9dea-95b7-4b08-d777-da10d4a1f330"
   },
   "outputs": [],
   "source": [
    "# assemble results for inspection\n",
    "pd.DataFrame(zip(token_text, token_lemmas, token_stop), columns=['Original Text', 'Stemmed Text', 'stopwords']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3Lb3KV_49mJ"
   },
   "source": [
    "### Step 1.2: The actual stemming process\n",
    "\n",
    "We actually stem the text in the corpus using a `nltk` based class, but we won't see very much of the process. Don't worry about the details here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kxDDSRgC4uf"
   },
   "outputs": [],
   "source": [
    "snowball_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class SBStemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(SBStemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([snowball_stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf_vectorizer = SBStemmedCountVectorizer(max_df = 0.9, min_df=0.01, analyzer=\"word\", stop_words='english') #max min required occurrence of word, english stop words\n",
    "tf = tf_vectorizer.fit_transform(data_pros_cleaned) # vectorize data (learn the vocabulary dictionary and return term-document matrix)\n",
    "voc = tf_vectorizer.get_feature_names() # extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8EiZn9DGg0K"
   },
   "source": [
    "## Step 2: Run LDA Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyper-parameter tuning** is an essential step in using any ML algorithm and it is one of the main steps prior to actually run the LDA model. \n",
    "\n",
    "We will not discuss how parameters are tuned here, but you can see some details further below in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jk45e0A7WgzF"
   },
   "source": [
    "**Select optimal topic number, K**\n",
    "\n",
    "Based on the result of a computation-heavy step that has been already run in background (coherence maximization algorithm to tune number of topics K), we will select the optimal number of topics to fit the topic model on the Glassdoor reviews available for Chimera.\n",
    "\n",
    "While we could be running the topic model with an arbitrary number of topics, tuning such a parameter is very important, as it (1) favours the accuracy of how data are represented as topics, and (2) increases the replicability of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9skI5ZCWpmD"
   },
   "outputs": [],
   "source": [
    "optimal_topics_pros = 157  # For cons: 207"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfSceCuqGzst"
   },
   "source": [
    "**Run LDA topic model on review text with optimal topic number**\n",
    "\n",
    "We will fit the LDA topic model on the \"pros\" and \"cons\" sections of the Chimera text reviews, separately. \n",
    "\n",
    "LDA takes as an input a matrix of documents X words (terms), generated through the vectorization stage. \n",
    "\n",
    "You can find more details about how to run LDA using the sklearn library (e.g., about the meaning of the various parameters in the function, and how to best tune those) [here](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_0uFnnpGqup"
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=optimal_topics_pros, max_iter=200, learning_method='batch', learning_offset=10.,evaluate_every=2,random_state=1234)\n",
    "lda_fit = lda.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9Vn_CVVkDvV"
   },
   "source": [
    "### Step 2.1: LDA output - Topic-over-word matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5it9sUIfkewy"
   },
   "source": [
    "We are finally able to generate one of the two main outputs of the topic model: a **matrix summarizing the distribution of the topics** estimated by LDA **over the words forming the corpus vocabulary** (i.e., all the unique words that appear in the documents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaWSC4slk2PT"
   },
   "source": [
    "Let's start by **visualizing the vocabulary**, how big it is, and the words it comprises: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIqrXFkzj9RS",
    "outputId": "12c0548c-a8aa-48fc-e3e9-7456722aa2f1"
   },
   "outputs": [],
   "source": [
    "# print length of vocabulary list\n",
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKcdCb5pj_VD",
    "outputId": "ccd60851-7654-472d-bf6c-e7a4dac923d5"
   },
   "outputs": [],
   "source": [
    "# print words forming the vocabulary\n",
    "voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b__ydoXlM2Q"
   },
   "source": [
    "Let's now generate the **matrix summarizing the distributions of topics over words** and inspect it. \n",
    "\n",
    "As we will see below, this matrix can be used to interpret the meaning of the topics estimated by LDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lMZ7pEPojZVf",
    "outputId": "8abe6f5d-64a0-40b4-c62b-636c3bb00334"
   },
   "outputs": [],
   "source": [
    "# distribution of topics over words in vocabulary\n",
    "topics_words = lda_fit.components_ / lda_fit.components_.sum(axis=1)[:, np.newaxis]\n",
    "topics_words_df = pd.DataFrame(topics_words)\n",
    "topics_words_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "z_xjWTXkj2mm",
    "outputId": "bf06a64c-4f9b-42ae-c2a9-905649f5466e"
   },
   "outputs": [],
   "source": [
    "# inspect matrix\n",
    "topics_words_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyW60aafG6ys"
   },
   "source": [
    "### Step 2.2: LDA output - Document-over-topic matrix\n",
    "\n",
    "The second output of interest, which we are going to generate and inspect below, is a **matrix summarizing the distribution of documents** (i.e., employee reviews, in our context) **over the topics** (i.e., lists of words that, statistically, co-occur across the reviews more likely than others) estimated by LDA. \n",
    "\n",
    "As we will see below, this matrix can be used to compute summary statistics and to interpret the topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PWcrzlqG_RU"
   },
   "outputs": [],
   "source": [
    "# distribution of documents over topics\n",
    "docs_topics = lda.transform(tf)\n",
    "docs_topics_df = pd.DataFrame(data = docs_topics, index=None, columns=None, dtype=None, copy=False)\n",
    "docs_topics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "XlMz-fHIjvPX",
    "outputId": "1cef0122-3984-4420-9665-c7033d8c82bf"
   },
   "outputs": [],
   "source": [
    "# inspect matrix\n",
    "docs_topics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HFVX69xZThR"
   },
   "source": [
    "## Step 3: Use topic model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLk6edTec8u0"
   },
   "source": [
    "Once the topic model has been computed, we need to better understand how to use its outputs. \n",
    "\n",
    "We will focus on **interpreting the topics** (step 3.1) to map them into meaningful theoretical constructs.\n",
    "Further below in the notebook, you can find more on leveraging the document-over-topic matrix to **compute summary statistics** about culture (step 3.2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1: Interpreting the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1VyH0hug4Hb"
   },
   "source": [
    "Topic interpretation can be done in two ways: \n",
    "\n",
    "1.   Leveraging the **topic-over-word matrix**, find a coherent meaning across the words forming a topic (traditional approach, but not very replicable)\n",
    "2.   Leveraging the **document-over-topic matrix**, identify **salient topics** and **prototypical documents** to interpret them (more novel approach, improves reliability and replicability)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-_vTne1hqsq"
   },
   "source": [
    "#### Step 3.1.1: Interpret topics beased on distribution of topics over vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBjFhbIdiWLm"
   },
   "source": [
    "This approach consists in identifying the most relevant words from the vocabulary associated with each topics and find some shared meaning among them to generate a label for the topic.\n",
    "\n",
    "As you remember, in our example we have a vocabulary comprising 341 words, which we have inspected before. Each topic can be summarized as a distribution over the vocabulary, as we have seen already. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4t6KLFCnoMQ"
   },
   "source": [
    "**Extract main words per topic**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnKrzu86nRVo"
   },
   "source": [
    "Leveraging the topic-over-word distribution matrix, we can **visualize the main words associated with each topic** (e.g., top 10 words scoring the highest probability to belong to a topic). \n",
    "\n",
    "These can be used to label the topic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPATzxxapjbk"
   },
   "source": [
    "Let's create a function to visualize the top words (number to be specified) associated to each topic: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8AIGSPbnt_-"
   },
   "outputs": [],
   "source": [
    "def show_topics(vectorizer, lda_model, n_words):\n",
    "    keywords = np.array(tf_vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lutugR0p6bT"
   },
   "source": [
    "We can now **inspect the main 5 words associated to each topic** extracted by LDA and try to identify some coherent meaning they may convey. This would help to **generate a label** for the topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4vrHcifoaUi",
    "outputId": "3f036dc0-3d9a-41a9-9a7f-512bd8ed9402"
   },
   "outputs": [],
   "source": [
    "# generate topic-keyword association, to inspect\n",
    "topic_keywords = show_topics(tf_vectorizer, lda, n_words=10)\n",
    "topic_keywords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6avb-byHiPCD"
   },
   "source": [
    "#### Step 3.1.2: Interpret topics beased on salient topics and prototypical text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mqrXPFGqIWj"
   },
   "source": [
    "Although the approach proposed above to interpret topics based on they words they are mostly associated with is very common, it is quite subjective and not very replicable. \n",
    "\n",
    "Below, we will use an alternative method, which leverages the **distribution of documents over topics**, to interpret the topics algorithmically tuned by LDA. \n",
    "\n",
    "This approach consists in **identifying the most salient topics** in the corpus (i.e., the main topics many people are talking about), which one should interpret, and the **prototypical documents** associated with them, to be used to perform the topic interpretation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oroqr7swZuoH"
   },
   "source": [
    "**Identify most salient topics for interpretation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UgIg2BjIvbl"
   },
   "source": [
    "Those are the key topics in the corpus, which we should interpet. More formally, **salient topics** are those that have a large fraction of reviews from the corpus focused on those.\n",
    "\n",
    "The saliency parameter can be tuned, and sets the proportion of documents the topic must be feature in, beyond what can be expected by chance (1/K).\n",
    "\n",
    "Enter saliency parameter of your choice (must fall in [0,1] interval):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CE_L1I-rPcdX",
    "outputId": "f7c42459-9e9c-4b24-cd2a-6862f7427adb"
   },
   "outputs": [],
   "source": [
    "saliency_parameter = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVhUBLvGyFmW"
   },
   "source": [
    "The next block of code will tell us how many topics we'd need to interpret, as salient, based on our choice of `saliency_parameter` (again, don't worry too much about the details here!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rM8_3lonHG4t",
    "outputId": "4bbcf120-9bf4-4464-e195-89dfa855f74f"
   },
   "outputs": [],
   "source": [
    "docs_topics_masked = docs_topics_df.copy(deep='True')\n",
    "column_names = list(docs_topics_masked)\n",
    "\n",
    "for i in range(0, len(column_names)):\n",
    "    docs_topics_masked.loc[docs_topics_masked[i] > 1/optimal_topics_pros, i] = 1\n",
    "\n",
    "for i in range(0, len(column_names)):\n",
    "    docs_topics_masked.loc[docs_topics_masked[i] < 1, i] = 0\n",
    "\n",
    "topic_salience = docs_topics_masked.sum(axis=0)\n",
    "topic_salience_df = pd.DataFrame(topic_salience, columns= [\"reviews_pastThreshold\"])\n",
    "topic_salience_df = topic_salience_df.reset_index()\n",
    "topic_salience_df = topic_salience_df.rename(columns = {'index': 'topic_id'})\n",
    "topic_salience_df[\"percReviews\"] = topic_salience_df[\"reviews_pastThreshold\"] / docs_topics_df.shape[1]\n",
    "q = topic_salience_df[\"percReviews\"].quantile(saliency_parameter)\n",
    "topic_salience_df = topic_salience_df[topic_salience_df.percReviews >= q]\n",
    "\n",
    "salient_topics_perc = len(topic_salience_df)/ len(docs_topics_df)\n",
    "\n",
    "print (\"When considering the top \"+str(round((1-saliency_parameter)*100,2))+\" % of the distribution of documents over topics, the algorithm identifies \" \n",
    "       +str(round(salient_topics_perc *100,2))+\"% of total topics as most salient (equal to \" \n",
    "       +str(int(salient_topics_perc * len(list(docs_topics_df))))+\" topics).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ht9360GZH5Dx"
   },
   "source": [
    "**Extraction of prototypical reviews for interpretation and export**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwCeSClvUMJA"
   },
   "source": [
    "We need to choose a parameter *l* that a represents the minimal focus of a review on a topic for the review to be considered **prototypical for the topic** and thus included in the final topic interpretation.\n",
    "\n",
    "More on how *l* can be chosen can be found below in the additional parts.\n",
    "\n",
    "The code here finds the prototypical reviews for the different topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ee8nT_G9ARPY"
   },
   "outputs": [],
   "source": [
    "l = 0.5  # For cons, a value of 0.4-0.5 seems to work fine\n",
    "\n",
    "rep_docs = [] #list saving, for each topic, the index of those reviews which load more than l on the topic\n",
    "for column in docs_topics_df: \n",
    "    rep_docs.append(docs_topics_df[(docs_topics_df[column]>l)==True].index.to_numpy())\n",
    "    \n",
    "row_topic = []\n",
    "i = 0\n",
    "for row in rep_docs: \n",
    "    topic_id = i\n",
    "    if len(rep_docs[topic_id]) == 0:\n",
    "        row_topic.append([topic_id])\n",
    "    else: \n",
    "        for j in range (0,len(rep_docs[topic_id])):\n",
    "            review_text = data_pros_cleaned[rep_docs[topic_id][j]]\n",
    "            review_id = rep_docs[topic_id][j]\n",
    "            row_topic.append([topic_id, review_text, review_id])\n",
    "    i = i+1\n",
    "    \n",
    "#convert into df\n",
    "row_topic_df = pd.DataFrame(row_topic, columns=[\"topic_id\", \"review_text\", \"review_id\"])\n",
    "row_topic_df = row_topic_df.dropna(axis=0)\n",
    "\n",
    "#extract prototypical reviews only for most salient topics\n",
    "prototipycalDocs_salientTopics_df = row_topic_df.merge(topic_salience_df, how = 'inner', left_on = 'topic_id', right_on = 'topic_id')\n",
    "\n",
    "#extract only relevant column_names\n",
    "prototipycalDocs_salientTopics_df = prototipycalDocs_salientTopics_df[['topic_id', 'review_text', 'review_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "al7F6cdfRNEf",
    "outputId": "2cac672d-e655-4861-d9e1-c165caa4d330"
   },
   "outputs": [],
   "source": [
    "# inspection of prototypical documents associated to most salient topics\n",
    "prototipycalDocs_salientTopics_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYV6DODS0x_g"
   },
   "source": [
    "Export results to excel and download for inspection: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkf2ciVb0Xxm"
   },
   "outputs": [],
   "source": [
    "prototipycalDocs_salientTopics_df.to_excel(\"Chimera_TopicsToInterpret_Pros.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Additional steps in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before step 2: hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1riwo_USm2z"
   },
   "source": [
    "**Hyper-parameter tuning** is an essential step in using any ML algorithm and it is one of the main steps prior to actually run the LDA model. \n",
    "\n",
    "We will focus our attention on hyperparameters **alpha, i.e., the topic smoothing parameter**, and **K, i.e., the otpimal number of topics to run LDA**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxY8m8GqTPNh"
   },
   "source": [
    "**Understand the role of topic smoothing parameter, alpha**\n",
    "\n",
    "Alpha changes the \"spikiness\" of the document-over-topic distributions. We can see this through a simulation, in the following block of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6Ir4865VM_z"
   },
   "source": [
    "We create one new function for the purpose of the simulation. This simulates the structure of a topic model output (document-over-topic distribution matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQFcXMBeUkkd"
   },
   "outputs": [],
   "source": [
    "def create_alpha_vector(A, beta):\n",
    "    alpha_vector = beta*np.ones(A, dtype=np.int)\n",
    "    return alpha_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBK2YV-7VI0a"
   },
   "source": [
    "Let's set the parameters for the simulation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LT8yoAWHSmYh",
    "outputId": "676ab466-b52d-4f54-d9c4-3f30f62970af"
   },
   "outputs": [],
   "source": [
    "# number of topics (columns)\n",
    "K = 5\n",
    "# number of documents (rows)\n",
    "D = 3\n",
    "# alpha\n",
    "alpha = 0.9  # You can vary this between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxC1h8o4VvIi"
   },
   "source": [
    "And now we can simulate the topic model output to understand how alpha affects the distributions of documents over topics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "bx9SVowCVzfu",
    "outputId": "6c822634-7f5a-4891-9e37-e961fc6d42ab"
   },
   "outputs": [],
   "source": [
    "# simulate distributions of documents over topics\n",
    "docs_over_topics = np.random.dirichlet(create_alpha_vector(K, alpha), D)\n",
    "docs_over_topics_df = pd.DataFrame(docs_over_topics)\n",
    "docs_over_topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d4YJBuKUE6M"
   },
   "source": [
    "\n",
    "Usually, LDA is run with the default value alpha = 1/K, as we will also do in the example below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kv3daAuodSFa"
   },
   "source": [
    "**Understand the relationship between number of topics and topical coherence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpDrsM1ISNIb"
   },
   "source": [
    "Import modules needed for coherence computation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vV3CD_JeOw1H"
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jx_NxQ3pfa5E"
   },
   "source": [
    "Create functions needed for coherence computation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJ3G8aKDffI1"
   },
   "outputs": [],
   "source": [
    "# used inside the coherence function\n",
    "def jsd(p, q, base=np.e):\n",
    "    p = np.asarray(p)\n",
    "    q = np.asarray(q)\n",
    "    m = 1./2*(p + q)\n",
    "    return sp.stats.entropy(p,m, base=base)/2. +  sp.stats.entropy(q, m, base=base)/2.\n",
    "\n",
    "def coherence(probMatrix):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(0, len(probMatrix)): \n",
    "        jsd_list = []\n",
    "        for j in range(0, len(probMatrix)): \n",
    "            jsd_list.append(jsd(probMatrix[i], probMatrix[j]))\n",
    "            j = j+1\n",
    "        df[str(i)] = jsd_list\n",
    "    mask = np.ones(df.shape,dtype='bool')\n",
    "    mask[np.triu_indices(len(df))] = False\n",
    "    df_lower_diagonal = df[(df>-1)&mask]\n",
    "   \n",
    "    distance_list = []\n",
    "    i = 0 \n",
    "    for i in range(0, len(df)): \n",
    "        column_list = df_lower_diagonal[str(i)].values.tolist() \n",
    "        column_lower_diagonal_list = [x for x in column_list if (math.isnan(x) == False)]\n",
    "        for d in column_lower_diagonal_list: \n",
    "            distance_list.append(d)\n",
    "        i = i + 1\n",
    "    coherence = sum(distance_list) / float(len(distance_list))\n",
    "    return coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JcVGNBFeSP2"
   },
   "source": [
    "Enter topic number for which you want to assess the solution coherence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8-HJioCdRxG",
    "outputId": "380fdfaf-b45b-4c95-8a98-679803c74116"
   },
   "outputs": [],
   "source": [
    "topics_number = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run LDA with chosen numer of topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=topics_number, max_iter=200, learning_method='batch', learning_offset=10.,evaluate_every=2,random_state=1234)\n",
    "lda_fit = lda.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m08qYRR5ebVN"
   },
   "source": [
    "Compute and print average topical coherence associated with the chosen number of topics (I have pre-computed the comparison point, as it takes quite some time to run the LDA with all the relevant values of `K`): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0lYNZto5ejXP",
    "outputId": "1de63242-8a64-46a4-9486-bcbeb7a7644e"
   },
   "outputs": [],
   "source": [
    "topicsOverWords = lda_fit.components_ / lda_fit.components_.sum(axis=1)[:, np.newaxis]\n",
    "coherence_avg = coherence(topicsOverWords)\n",
    "\n",
    "if topics_number == 157:\n",
    "    print (\"You are at the optimal coherence solution: the average topical coherence is equal to 0.58.\")\n",
    "\n",
    "else: \n",
    "    print (\"When you run LDA with \"+str(topics_number)+\" topics, the average topic coherence in the solution is equal to \"+str(round(coherence_avg,2))+\".\\nRemember to benchmark this with the optimal coherence, equal to 0.58, obtained when running LDA with 157 topics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of step 3.1.2: Selection of parameter *l* to extract prototypical reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeuxKd6c5fub"
   },
   "source": [
    "The **parameter l** represents the minimal focus of a review on a topic for the review to be considered **prototypical for the topic** and thus included in the final topic interpretation.\n",
    "\n",
    "Choose the parameter *l* to maximize the percentage of interpretable topics in the solution, keeping in mind that you want to have a reasonable (i.e., not too high) number of reviews to screen, to label the topic content. The graph generated below as a function of *l* will help you making that choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0DzgerKHXvZ"
   },
   "outputs": [],
   "source": [
    "l_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "output_final = np.empty((0,6))\n",
    "\n",
    "for l in l_list: \n",
    "    rep_docs = [] #list saving, for each topic, the index of those reviews which load more than l on the topic\n",
    "    for column in docs_topics_df: \n",
    "        rep_docs.append(docs_topics_df[(docs_topics_df[column]>l)==True].index.to_numpy())\n",
    "     \n",
    "    row_topic = []\n",
    "    i = 0\n",
    "    for row in rep_docs: \n",
    "        topic_id = i\n",
    "        if len(rep_docs[topic_id]) == 0:\n",
    "            row_topic.append([topic_id])\n",
    "        else: \n",
    "            for j in range (0,len(rep_docs[topic_id])):\n",
    "                review_text = data_pros_cleaned[rep_docs[topic_id][j]]\n",
    "                review_id = rep_docs[topic_id][j]\n",
    "                row_topic.append([topic_id, review_text, review_id])\n",
    "        i = i+1\n",
    "    \n",
    "    row_topic_df = pd.DataFrame(row_topic, columns=[\"topic_id\", \"review_text\", \"review_id\"])   \n",
    "    \n",
    "    #generate summary stats for nr of prototypical reviews associated to topics\n",
    "    avg = row_topic_df.groupby(['topic_id']).topic_id.agg('count').describe()[1]\n",
    "    stdDev = row_topic_df.groupby(['topic_id']).topic_id.agg('count').describe()[2]\n",
    "    minVal = row_topic_df.groupby(['topic_id']).topic_id.agg('count').describe()[3]\n",
    "    maxVal = row_topic_df.groupby(['topic_id']).topic_id.agg('count').describe()[7]\n",
    "    \n",
    "    #compute M_k, i.e. number of documents extracted for each topic k for interpretation \n",
    "    #(to form prototypical text)\n",
    "    m_k = row_topic_df.groupby(['topic_id']).topic_id.agg('count').to_frame('M_k').reset_index()\n",
    "     \n",
    "    t = 1/l #threshold for topics to be interpreted (they need to have M_k>=t)\n",
    "    #keep only interpretable topics, given the specific value of l\n",
    "    m_k = m_k.drop(m_k[m_k.M_k < t].index)\n",
    "    \n",
    "    #compute percentage of interpretable topics\n",
    "    perc_int = len(m_k)/optimal_topics_pros\n",
    "\n",
    "    #save results\n",
    "\n",
    "    output=np.zeros((1,6))\n",
    "\n",
    "    #store results per firm   \n",
    "    output[0,0] = l\n",
    "    output[0,1] = perc_int\n",
    "    output[0,2] = avg\n",
    "    output[0,3] = stdDev\n",
    "    output[0,4] = minVal\n",
    "    output[0,5] = maxVal\n",
    "    \n",
    "    output_final = np.append(output_final, output, axis = 0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFvMPxC9yRQv"
   },
   "source": [
    "The next block of code will plot the percentage of interpretable topics as a function of *l*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "ROoY-W_EQIrd",
    "outputId": "5d42ea42-8cc4-40f7-ef84-b50ab470679a"
   },
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(output_final, columns = ['l', 'interpretableTopics', 'avgRev', 'stDevRev', 'minRev', 'maxRev'])\n",
    "output_df.plot.line(x='l', y='interpretableTopics', color='grey')\n",
    "\n",
    "plt.xlabel('l Threshold')\n",
    "plt.ylabel('% of interpretable topics')\n",
    "plt.title('Interpretable topics as function of l')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wz68k15GybpM"
   },
   "source": [
    "The next block of code will plot relevant statistics about the numnber of reviews associated with each salient topic as a funcion of *l*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjIyl8AiQIUd"
   },
   "outputs": [],
   "source": [
    "output_df.plot.line(x='l', y=['avgRev', 'stDevRev', 'minRev', 'maxRev'])\n",
    "\n",
    "plt.xlabel('l Threshold')\n",
    "plt.ylabel('Prototypical reviews')\n",
    "plt.title('Summary statistics: Prototypical reviews per topic')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set optimal l based on results graphed above**\n",
    "\n",
    "Your objective is to (1) maximize the percentage of topics from the corpus you can interpret, while (2) not having too many reviews to analyse for each topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_YRLxwXZhG1"
   },
   "source": [
    "### Step 3.2: Compute culture metrics on document-over-topic matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twQ6-SMkfgRx"
   },
   "source": [
    "The document-over-topic matrix can be used to compute distance metrics across the documents in the corpus, and to understand the breadth of content covered by each document (as in the Herfindahlâ€“Hirschman Index).\n",
    "\n",
    "We compute three metrics of interest: average similarity, average focus, and average cross-entropy of the document-over-topic probability matrix. \n",
    "\n",
    "These three metrics summarize the shape of the matrix and tell us whether **people are similar in the content they write about** in their review and whether **they write many or few topics** in their document. \n",
    "\n",
    "Keep in mind that these metrics are not very useful, in absolute terms, and are generally used to compare corpora (summarizing, for instance, different organizations, or groups of individuals) among each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akqo2p9Qfxvf"
   },
   "source": [
    "Let's first define functions to compute the metrics, taking the document-over-topic probability matrix as input: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nFyAwGBb6Ip"
   },
   "outputs": [],
   "source": [
    "def similarity_func(probMatrix): #takes in input array \n",
    "    #Transform probMatrix_df into 2D array\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for i in range(0, len(probMatrix)): \n",
    "        jsd_list = []\n",
    "        for j in range(0, len(probMatrix)): \n",
    "            jsd_list.append(jsd(probMatrix[i], probMatrix[j]))\n",
    "            j = j+1\n",
    "        df[str(i)] = jsd_list\n",
    "\n",
    "    mask = np.ones(df.shape,dtype='bool')\n",
    "    mask[np.triu_indices(len(df))] = False\n",
    "    df_lower_diagonal = df[(df>-1)&mask]\n",
    "    \n",
    "    distance_list = []\n",
    "    i = 0 \n",
    "    for i in range(0, len(df)): \n",
    "    #Transform each column of df_lower_diagonal into list\n",
    "        column_list = df_lower_diagonal[str(i)].values.tolist()\n",
    "        #Drop nan values from column_list - to retain only actual values from lower diagonal \n",
    "        column_lower_diagonal_list = [x for x in column_list if (math.isnan(x) == False)]\n",
    "        for d in column_lower_diagonal_list: \n",
    "            distance_list.append(d)\n",
    "        i = i + 1\n",
    "    sim = 1-(sum(distance_list) / float(len(distance_list))) #this will return similarity, instead of comph\n",
    "    return sim\n",
    "\n",
    "def focus_func(probMatrix_df):  #takes in input pandas df \n",
    "    N = probMatrix_df.shape[0]\n",
    "    probMatrix = probMatrix_df.values\n",
    "    foc = (sum(map(sum, np.square(probMatrix))))/N\n",
    "    return foc \n",
    "\n",
    "def ent_avg(probMatrix):\n",
    "    entropy_list = []\n",
    "    for i in range(len(probMatrix)): \n",
    "        entropy_list.append(sp.stats.entropy(probMatrix[i]))\n",
    "    return np.mean(entropy_list)\n",
    "\n",
    "def cross_entropy(p, q):\n",
    "    for i in range(len(p)):\n",
    "        p[i] = p[i]+1e-12\n",
    "    for i in range(len(q)):\n",
    "        q[i] = q[i]+1e-12\n",
    "\n",
    "    return -sum([p[i] * np.log2(q[i]) for i in range(len(p))])\n",
    "\n",
    "def avg_crossEnt_func(probMatrix): #takes in input array \n",
    "\n",
    "    crossEntropy_list = []\n",
    "    for i in range(len(probMatrix)):\n",
    "        for j in range(len(probMatrix)): \n",
    "            if i > j:\n",
    "                crossEntropy_local_list = []\n",
    "                crossEntropy_local_list.append(cross_entropy(probMatrix[i], probMatrix[j]))\n",
    "                crossEntropy_local_list.append(cross_entropy(probMatrix[j], probMatrix[i]))\n",
    "                crossEntropy_list.append(np.mean(crossEntropy_local_list))\n",
    "        \n",
    "    return np.mean(crossEntropy_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the functions may take some time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score = similarity_func(docs_topics)\n",
    "focus_score = focus_func(docs_topics_df)\n",
    "ace_score = avg_crossEnt_func(docs_topics)\n",
    "print(similarity_score)\n",
    "print(focus_score)\n",
    "print(ace_score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TopicModeling_PhDWorkshop_2021",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dtvc_env",
   "language": "python",
   "name": "dtvc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
