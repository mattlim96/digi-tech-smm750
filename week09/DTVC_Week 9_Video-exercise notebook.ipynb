{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import graphviz as gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: K-means on the happiness index\n",
    "\n",
    "We use the 2019 happiness index dataset available here: https://www.kaggle.com/unsdsn/world-happiness \n",
    "We have removed the columns giving us the ranking and the score of each country and just kept the bare-bones indicators. The goal is to cluster the countries into countries with similar attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "happiness=pd.read_csv(\"countries_indicators.csv\")\n",
    "happiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Introduction to K-means\n",
    "\n",
    "We start with a basic version of K-means to just get used to the set-up in Python. We pick the number of clusters to be equal to 3 and we do no pre-processing. We drop the country or region column as well as the country code for this purpose as K-means works on numerical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness_quant=happiness.drop(columns=[\"Country or region\",\"Code\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run the code below to run K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3).fit(happiness_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run the two snippets of code below. What do you think they are giving us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run the code below to plot the three clusters on a map. Make sure to go look there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=kmeans.labels_\n",
    "df=pd.DataFrame(labels,columns=['Cluster'])\n",
    "df[\"Code\"]=happiness.Code\n",
    "\n",
    "from pygal.maps.world import World\n",
    "from IPython.display import SVG, display\n",
    "\n",
    "wm = World()\n",
    "wm.force_uri_protocol = 'http'\n",
    "\n",
    "cluster0=pd.Series.to_numpy(df[df[\"Cluster\"]==0][\"Code\"])\n",
    "cluster1=pd.Series.to_numpy(df[df[\"Cluster\"]==1][\"Code\"])\n",
    "cluster2=pd.Series.to_numpy(df[df[\"Cluster\"]==2][\"Code\"])\n",
    "\n",
    "wm.add('Cluster 0', cluster0)\n",
    "wm.add('Cluster 1',cluster1)\n",
    "wm.add('Cluster 2',cluster2)\n",
    "display(SVG(wm.render()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Scaling the data\n",
    "\n",
    "We now preprocess the data by scaling it.\n",
    "\n",
    "1. Run the code below. In terms of absolute values, which features dominate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness.hist(figsize=[5,8])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Rerun the clustering with scaling code below. Which countries have changed clusters? Can you come up with an explanation of why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "happiness_quant=preprocessing.scale(happiness_quant)\n",
    "kmeans = KMeans(n_clusters=3).fit(happiness_quant)\n",
    "\n",
    "#plotting\n",
    "labels=kmeans.labels_\n",
    "df=pd.DataFrame(labels,columns=['Cluster'])\n",
    "df[\"Code\"]=happiness.Code\n",
    "\n",
    "wm = World()\n",
    "wm.force_uri_protocol = 'http'\n",
    "\n",
    "cluster0=pd.Series.to_numpy(df[df[\"Cluster\"]==0][\"Code\"])\n",
    "cluster1=pd.Series.to_numpy(df[df[\"Cluster\"]==1][\"Code\"])\n",
    "cluster2=pd.Series.to_numpy(df[df[\"Cluster\"]==2][\"Code\"])\n",
    "\n",
    "wm.add('Cluster 0', cluster0)\n",
    "wm.add('Cluster 1',cluster1)\n",
    "wm.add('Cluster 2',cluster2)\n",
    "display(SVG(wm.render()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness.loc[happiness['Country or region'] == \"Saudi Arabia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness.loc[happiness['Country or region'] == \"United Kingdom\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Impact of random initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run the code below twice, saving the map each time under a different name (the output maps should be found within the same folder as the source file). Are the two maps obtained the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness_quant=preprocessing.scale(happiness_quant)\n",
    "kmeans = KMeans(n_clusters=4,n_init=1).fit(happiness_quant)\n",
    "\n",
    "#plotting\n",
    "labels=kmeans.labels_\n",
    "df=pd.DataFrame(labels,columns=['Cluster'])\n",
    "df[\"Code\"]=happiness.Code\n",
    "\n",
    "wm = World()\n",
    "wm.force_uri_protocol = 'http'\n",
    "\n",
    "cluster0=pd.Series.to_numpy(df[df[\"Cluster\"]==0][\"Code\"])\n",
    "cluster1=pd.Series.to_numpy(df[df[\"Cluster\"]==1][\"Code\"])\n",
    "cluster2=pd.Series.to_numpy(df[df[\"Cluster\"]==2][\"Code\"])\n",
    "cluster3=pd.Series.to_numpy(df[df[\"Cluster\"]==3][\"Code\"])\n",
    "\n",
    "wm.add('Cluster 0', cluster0)\n",
    "wm.add('Cluster 1',cluster1)\n",
    "wm.add('Cluster 2',cluster2)\n",
    "wm.add('Cluster 3',cluster3)\n",
    "wm.render_to_file('map2.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The code above is exactly the same in both cases. What leads to the differences observed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Choosing the right K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run the code below. What are we getting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia_K=[]\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(happiness_quant)\n",
    "    inertia_K.append(kmeanModel.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Plot inertia_K as a function of K using plt.plot. What value would you choose for K?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K,inertia_K)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Hierarchical clustering on the happiness index\n",
    "\n",
    "Exceptionally, we use scipy rather than scikit-learn as scikit-learn does not have an easy module for drawing dendograms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Without scaling\n",
    "\n",
    "We start off by importing the dataset once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness=pd.read_csv(\"countries_indicators.csv\")\n",
    "happiness_quant=happiness.drop(columns=[\"Country or region\",\"Code\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Draw a dendrogram in the with \"average\" linkage. How many clusters do you think we should have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(happiness_quant,method='average')\n",
    "\n",
    "dendrogram(Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Draw another dendogram, this time with \"complete\" linkages. This time, how many clusters do you think we should have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(happiness_quant,method='complete')\n",
    "\n",
    "dendrogram(Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the code below to find the cluster assignment for complete linkage that gives the most balanced graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=fcluster(Z, 3, criterion='maxclust') #returns clustering with 3 clusters, can also specify max distance you are okay with\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use this code below to obtain the map of the world with this clustering. What do you think of the quality of the clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(labels,columns=['Cluster'])\n",
    "df[\"Code\"]=happiness.Code\n",
    "\n",
    "from pygal.maps.world import World\n",
    "from IPython.display import SVG, display\n",
    "wm = World()\n",
    "wm.force_uri_protocol = 'http'\n",
    "\n",
    "cluster1=pd.Series.to_numpy(df[df[\"Cluster\"]==1][\"Code\"])\n",
    "cluster2=pd.Series.to_numpy(df[df[\"Cluster\"]==2][\"Code\"])\n",
    "cluster3=pd.Series.to_numpy(df[df[\"Cluster\"]==3][\"Code\"])\n",
    "\n",
    "wm.add('Cluster 1',cluster1)\n",
    "wm.add('Cluster 2',cluster2)\n",
    "wm.add('Cluster 3', cluster3)\n",
    "\n",
    "display(SVG(wm.render()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Using scaling\n",
    "\n",
    "We now scale the dataset and repeat the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness=pd.read_csv(\"countries_indicators.csv\")\n",
    "happiness_quant=happiness.drop(columns=[\"Country or region\",\"Code\"])\n",
    "\n",
    "from sklearn import preprocessing\n",
    "happiness_quant=preprocessing.scale(happiness_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Draw a dendrogram in the case where the linkage is average and when it is complete. How many clusters do you think we should have for both cases? Which type of linkage would you prefer to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(happiness_quant,method='average')\n",
    "\n",
    "dendrogram(Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(happiness_quant,method='complete')\n",
    "\n",
    "dendrogram(Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the code below to find the cluster assignment for complete linkage that gives the most balanced graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=fcluster(Z, 3, criterion='maxclust')\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use this code below to obtain the map of the world with this clustering. What do you think of the quality of the clusters in contrast with K-means and hierarchical clustering without scaling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(labels,columns=['Cluster'])\n",
    "df[\"Code\"]=happiness.Code\n",
    "\n",
    "from pygal.maps.world import World\n",
    "wm = World()\n",
    "wm.force_uri_protocol = 'http'\n",
    "\n",
    "cluster1=pd.Series.to_numpy(df[df[\"Cluster\"]==1][\"Code\"])\n",
    "cluster2=pd.Series.to_numpy(df[df[\"Cluster\"]==2][\"Code\"])\n",
    "cluster3=pd.Series.to_numpy(df[df[\"Cluster\"]==3][\"Code\"])\n",
    "\n",
    "wm.add('Cluster 1',cluster1)\n",
    "wm.add('Cluster 2',cluster2)\n",
    "wm.add('Cluster 3', cluster3)\n",
    "\n",
    "display(SVG(wm.render()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: the Daily Kos dataset (optional)\n",
    "\n",
    "For this exercise, we are considering a dataset called the `dailykos` dataset. It contains data on 3,430 news articles or blogs that have been posted on *Daily Kos*, an American political blog that publishes news and opinion articles written from a progressive point of view. These articles were posted in 2004, leading up to the United States Presidential Election. The leading candidates were incumbent President George W. Bush (republican) and John Kerry (democratic). Foreign policy was a dominant topic of the election, specifically, the 2003 invasion of Iraq. Our goal is to cluster the articles that appear in the dataset.\n",
    "\n",
    "Note: Each observation is a news article (with 3,430 total) and each feature is a word that has appeared in at least 50 of these articles (with 1,545 words in total). The values are then then number of times that the given word has appeared in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailykos=pd.read_csv(\"dailykos.csv\")\n",
    "dailykos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Hierarchical clustering\n",
    "\n",
    "1. Do we need to scale the dataset here? Why/why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We use `scipy` to obtain the dendrogram for this section. We are going to use `method=\"ward\"` here as it gives rise to the best results. Generate the dendrogram for this dataset (note: due to the number of words and articles, this may take a while). In light of the application and of the dendrogram, how many clusters would you pick?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate the labels of each datapoint. Which one is the largest cluster? Which one is the smallest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Add a new column to dailykos which contains the labels. Then, filter the dataset based on these labels: for example, restrict yourselves to those rows which correspond to `labels==1`. For those rows, take a look at the 5 words that appear most often on average. Are there some clusters that stand out in terms of topic? How many observations are there in each one? The code for the first label is provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. K-means clustering\n",
    "\n",
    "We now give K-means clustering a try. We keep the same number (7) of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailykos=pd.read_csv(\"dailykos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the code introduced before, run a K-means algorithm on dailykos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=7).fit(dailykos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Add a \"label\" column to dailykos again. (note: kmeans creates clusters 0-6, unlike hierarchical clustering, which gives us clusters 1-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using a similar method to above, obtain the top 5 words for each cluster. For those clusters that you had assigned a topic too, can you find them again in the k-means clusters? Are you maybe even able to map other clusters obtained via k-means to the hierarchical clusters above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtvc_env",
   "language": "python",
   "name": "dtvc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
