{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import graphviz as gp\n",
    "import statsmodels.api as sm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Running CART on the Titanic dataset\n",
    "\n",
    "The goal here is to run CART on the full Titanic dataset (we take the training dataset provided by Kaggle and split it three ways into testing, training, validation - the testing set provided by Kaggle does not have the \"Survived\" category making it impossible for us to measure the quality of our model). This dataset has already been fully pre-processed --- note that this data prepreprocessing has been carried separately on each dataset to avoid train-test contamination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Titanic_train=pd.read_csv(\"Titanic_train_cleaned.csv\")\n",
    "Titanic_test=pd.read_csv(\"Titanic_test_cleaned.csv\")\n",
    "Titanic_validation=pd.read_csv(\"Titanic_val_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain=Titanic_train[[\"Survived\"]]\n",
    "Xtrain=Titanic_train.drop(columns=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yvalidation=Titanic_validation[[\"Survived\"]]\n",
    "Xvalidation=Titanic_validation.drop(columns=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest=Titanic_test[[\"Survived\"]]\n",
    "Xtest=Titanic_test.drop(columns=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For a number of maximum leaves equal to 6, use the following snippet of code to fit a CART to the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier_DT = DecisionTreeClassifier(max_leaf_nodes = 6)\n",
    "classifier_DT.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Graph the decision tree using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "dot_data = export_graphviz(classifier_DT, feature_names = Xtrain.columns, filled = True, rounded = True, class_names=[\"Died\",\"Survived\"])\n",
    "graph = gp.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How accurate is this decision tree on the validation data? Generate the predicted values using the code below and then compute the acccuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier_DT.predict(Xvalidation)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "score=accuracy_score(yvalidation,y_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We are now going to use the validation set to optimize how many leaves we should have in our tree. The goal here is to use a for loop to iterate over possible values of the maximum number of leaves, fit the model each time on the *training set* and obtain the `accuracy_score` on the *validation set*. Use the snippet of data below to get started. For comparison, also find the `accuracy_score` on the *training set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max_leaf_nodes = range(2,60) # Lets train the models with 2, 3, 4, ... 60 leafs\n",
    "train_array = []\n",
    "validation_array = []\n",
    "\n",
    "for n in n_max_leaf_nodes:\n",
    "    \n",
    "    #insert here the code to train the Decision Tree on the training set\n",
    "    \n",
    "    #insert here the code that makes predictions for both the validation and the training sets \n",
    "    \n",
    "    #insert here the code that gives us the accuracy of the model on the validation and training sets\n",
    "    #store the accuracy values as validation_score and train_score\n",
    "\n",
    "    train_array.append([n,train_score])\n",
    "    validation_array.append([n,validation_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We can now plot both accuracy levels over the chosen number of nodes. Which number of max_leaf_nodes would you feel comfortable picking?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = pd.DataFrame(validation_array)\n",
    "plt.scatter(array[0],array[1])\n",
    "\n",
    "array_train = pd.DataFrame(train_array)\n",
    "plt.scatter(array_train[0],array_train[1])\n",
    "plt.legend(['Validation set','Training set'])\n",
    "plt.xlabel(\"Number of leaves\",fontsize=15)\n",
    "plt.ylabel(\"Accuracy\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Retrain your model using this max_leaf_nodes on the full training and validation set. Plot the corresponding tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrainval=Xtrain.append(Xvalidation)\n",
    "ytrainval=ytrain.append(yvalidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert here code to retrain your model on the training + validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert here code to graph tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What is the accuracy_score on the testing set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Pulling up the Titanic Movie dataset, what do you predict for Rose and Jack in terms of survival? If you have seen the movie, does it agree with the outcomes they meet there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Titanic_movie=pd.read_csv(\"Titanic_movie.csv\")\n",
    "Titanic_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_DT.predict(Titanic_movie.drop(columns=\"Name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: The Boston houseprices dataset - CART and more advanced tools for regression (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we use the Boston crime dataset from, e.g., http://lib.stat.cmu.edu/datasets/boston ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston=sm.datasets.get_rdataset(\"Boston\",\"MASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_data=boston.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As usual, take a look at the header of the dataset and make sure you understand the features before you begin. Are there any duplicates? Any empty values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We will be attempting to predict the median value of the houses based on the other features using CART. In class, we saw how to use decision trees to *classify*, now our goal is to perform *regression*. To do this create the appropriate feature dataset `X` and the appropriate labels `y`. Then separate these into train/validation/test in proportions of 50/25/25 using `test_train_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using `DecisionTreeRegressor` which works very similarly to `DecisionTreeClassifier`, plot a decision tree for this problem based with `max_leaf_nodes=8`. Try and interpret the tree: which variables seem to condition the value of property? What information is given for each node of the tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Fit the decision tree obtained to `Xvalidation` to obtain `ypredict`. Compute the MSE between `ypredict` and `yvalidation` using `mean_squared_error`. Take a look at its square root. What do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Similarly to what was done in lecture, (1) train this model for many different values of `max_leaf_nodes`, (2) predict the values taken on the validation set, (3) compute the MSE between predicted values and real values, (4) append this value to the `array` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max_leaf_nodes = range(2,40) # Lets train the models with 2, 3, 4, ... 40 leaves\n",
    "\n",
    "array = []\n",
    "\n",
    "for n in n_max_leaf_nodes:\n",
    "    \n",
    "    #insert here the code to train the regressor on the dataset for varying levels of max_leaf_nodes   \n",
    "    \n",
    "    #insert here the code that gives us the accuracy of the model on the validation set \n",
    "    \n",
    "    array.append([n,score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Let's contrast this with the accuracy when evaluated on the training set. Obtain a second set `array_train` with the `accuracy_score` for each `max_leaf_nodes` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max_leaf_nodes = range(2,40) # Lets train the models with 2, 3, 4, ... 40 leafs\n",
    "array_train= []\n",
    "\n",
    "for n in n_max_leaf_nodes:\n",
    "    \n",
    "    #insert here the code to train the regressor on the dataset for varying levels of max_leaf_nodes\n",
    "        \n",
    "    #insert here the code that gives us the accuracy of the model on the training set \n",
    "    \n",
    "    array_train.append([n,score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Plot the predicted scores on the training set and validation set as a function of the number of leaves. What do you observe? Does that make sense? How many leaves would you feel comfortable picking?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = pd.DataFrame(array)\n",
    "plt.scatter(array[0],array[1])\n",
    "\n",
    "array_train = pd.DataFrame(array_train)\n",
    "plt.scatter(array_train[0],array_train[1])\n",
    "plt.legend(['Validation set','Training set'])\n",
    "plt.xlabel(\"number of leaves\",fontsize=15)\n",
    "plt.ylabel(\"MSE\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Retrain the model on the training and validation sets with 13 leaves. What is the value of the square root of the MSE on the testing set? Denote this value by `RMSE_DT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrainval=Xtrain.append(Xvalidation)\n",
    "ytrainval=ytrain.append(yvalidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert here code to retrain your model on the training + validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert here code to graph the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert here code to obtain predictions and compute the RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. When you look at this tree, what features play a large role in determining house prices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are basically 4 (ordered from maybe most important to least): lstat (% lower status population), rm (average number of rooms per dwelling), dis (weighted distance to employment centers), nox (air quality), tax (full-property tax rate). Note that e.g. higher taxes lead to a lower price for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Bagging / Random Forests / Gradient Boosting [optional]\n",
    "\n",
    "In this part, we contrast bagging/random forests/gradient boosting and see which one does the best. It stands to reason that these three methods should at least improve on what we have seen so far. \n",
    "\n",
    "We will throughout use the fact that max_leaf_nodes=13 and train our models on `Xtrainval, ytrainval`.\n",
    "\n",
    "We start first with Bagging/Random Forests and then move onto Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We use `RandomForestRegressor` for both, tweaking only the parameter `max_features`. Explain why this can be done and what we should set `max_features` to for it to be bagging, and what it should be set equal to for it to be random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Let's start with Bagging. Use `RandomForestRegressor` in exactly the same way as `DecisionTreeRegressor` to fit the model on `Xtrainval,ytrainval` with `max_leaf_nodes=13` and `max_features` appropriately set.\n",
    "\n",
    "Hint: you may want to use `ytrainval[\"medv\"]` instead of `ytrain` to avoid the error message you may be getting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the model you've just obtained to predict the values on `Xtest`. Then, compute the MSE between the predicted values and `ytest`. What is its square root? Denote this by `RMSE_Bagging`. Does it improve on `RMSE_DT`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Let's move onto random forests. Use `RandomForestRegressor` in exactly the same way as `DecisionTreeRegressor` to fit the model on `Xtrainval,ytrainval` with `max_leaf_nodes=13` and `max_features` appropriately set.\n",
    "\n",
    "Hint: you may want to use `ytrainval[\"medv\"]` instead of `ytrain` to avoid the error message you may be getting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use the model you've just obtained to predict the values on `Xtest`. Then, compute the MSE between the predicted values and `ytest`. What is its square root? Denote this by `RMSE_RF`. Does it improve on `RMSE_DT`? on `RMSE_Bagging`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Finally, for gradient boosting, use `GradientBoostingRegressor` (which again has a very similar syntax to `DecisionTreeRegressor`) to fit the model on `Xtrainval,ytrainval` with `max_leaf_nodes=13`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use the model you've just obtained to predict the values on `Xtest`. Then, compute the MSE between the predicted values and `ytest`. What is its square root? Denote this by `RMSE_Boost`. Does it improve on `RMSE_DT`? on `RMSE_Bagging`? on `RMSE_RF`? What do you think is the best model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtvc_env",
   "language": "python",
   "name": "dtvc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
