{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "897d6a28",
   "metadata": {},
   "source": [
    "# Automatically scrape job postings of a competitor\n",
    "\n",
    "We will now see scraping in action. Imagine you are working in HR for a major retailer. Your boss asks you to monitor the strategic hiring decisions of your close competitors. Naturally, you cannot go and call them up - but you could take a look at their job postings to see ($i$) how much they are hiring, ($ii$) what types of positions they are hiring for.\n",
    "\n",
    "Now, you could log onto their website every day, see what job postings there are, compare that with the job postings from before, and save the relevant data. But why go through so much effort if we can just automate the task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da075fd",
   "metadata": {},
   "source": [
    "## 1. Using BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fb50f",
   "metadata": {},
   "source": [
    "The first example relies purely on what we have learned about BeautifulSoup and Requests (and a bit of Pandas!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9ba464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bd6bf4",
   "metadata": {},
   "source": [
    "We are searching for positions in the head office of Aldi. On the website, we see that there are different types of head office positions, each with their own website. Let's get the links to those sub-sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.aldirecruitment.co.uk/head-office\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f146c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for link in soup.findAll('a'):\n",
    "    links.append(link.get('href'))\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4cfbfd",
   "metadata": {},
   "source": [
    "We want to get only the links to actual job postings, so we have to clean the results somewhat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6292c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_links = []\n",
    "for link in links:\n",
    "    if link != None and link != '/head-office/' and link.startswith('/head-office/'):\n",
    "        cleaned_links.append(link.replace('/head-office',''))\n",
    "links = cleaned_links\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd72effa",
   "metadata": {},
   "source": [
    "Let's see how many postings there are on one of the sub-sites. For this, we have to find the right tags, using their class argument. Again, inspecting the site is very important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ff0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url + links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2014535",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_url = url + links[0]\n",
    "page = requests.get(category_url)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "postings = soup.findAll(\"div\", class_=\"c-career--dropdown\")\n",
    "len(postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c6b051",
   "metadata": {},
   "source": [
    "We now extract some information from the actual position: the job title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2a40a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = postings[0].find(\"div\", class_=\"c-career--dropdown__content\").find('h2')\n",
    "print(title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a5bb9e",
   "metadata": {},
   "source": [
    "Aside from the title and the text description (which we will ignore in this example, but which can hold extremely useful information), there are some key details about the job, such as the work time and the salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6210c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "details = postings[0].findAll(\"div\", class_=\"c-job-details__content\")\n",
    "print(details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce29c5f7",
   "metadata": {},
   "source": [
    "We definitely want to get the salary information. Sometime, the text gives multiple values, so let's make sure to save the lowest and the highest value (of course, multiple values may be due to changes over time or for different starting requirements - we can adapt our scraper to capture arbitrary complexity later on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c550ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "detail = details[0]\n",
    "detail_text = detail.find('div', class_=\"c-job-details__text\").text\n",
    "print(detail_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22a12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = detail_text.replace(',','')\n",
    "temp = temp.replace('-','')\n",
    "temp = temp.split()\n",
    "salary_numbers = [float(s[1:]) for s in temp if s.startswith('£')]\n",
    "ub = max(salary_numbers)\n",
    "lb = min(salary_numbers)\n",
    "print(ub)\n",
    "print(lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01972c7",
   "metadata": {},
   "source": [
    "Let's also try to capture the weekly working hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0469241",
   "metadata": {},
   "outputs": [],
   "source": [
    "detail = details[2]\n",
    "detail_text = detail.find('div', class_=\"c-job-details__text\").text\n",
    "for s in detail_text.split():\n",
    "    if '-hour' in s:\n",
    "        work_time = s\n",
    "        work_time = int(work_time.replace('-hour',''))\n",
    "print(work_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6438d9",
   "metadata": {},
   "source": [
    "The following code combines our extraction of job details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fc960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "details = postings[0].findAll(\"div\", class_=\"c-job-details__content\")\n",
    "for detail in details:\n",
    "    detail_title = detail.find('span', class_=\"c-job-details__title\").text\n",
    "    detail_text = detail.find('div', class_=\"c-job-details__text\").text\n",
    "    if detail_title == 'Salary':\n",
    "        temp = detail_text.replace(',','')\n",
    "        temp = temp.replace('-','')\n",
    "        temp = temp.split()\n",
    "        salary_numbers = [float(s[1:]) for s in temp if s.startswith('£')]\n",
    "        ub = max(salary_numbers)\n",
    "        lb = min(salary_numbers)\n",
    "    elif detail_title == 'Hours and benefits':\n",
    "        for s in detail_text.split():\n",
    "            if '-hour' in s:\n",
    "                work_time = s\n",
    "                work_time = int(work_time.replace('-hour',''))\n",
    "print(ub)\n",
    "print(lb)\n",
    "print(work_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e069e2dd",
   "metadata": {},
   "source": [
    "Finally, we are putting it all together into a simple-to-call function that returns a data frame of job postings. We have to make a few adjustments to avoid errors. These are marked with comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b8c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_aldi_jobs(starting_page = 'head-office'):\n",
    "    url = \"https://www.aldirecruitment.co.uk/\" + starting_page\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    links = []\n",
    "    for link in soup.findAll('a'):\n",
    "        new_link = link.get('href')\n",
    "        if new_link != None and new_link.startswith('/head-office/'):\n",
    "            new_link = new_link.replace('/head-office','')\n",
    "            if new_link != '/':\n",
    "                links.append(new_link)\n",
    "    \n",
    "    department = []\n",
    "    titles = []\n",
    "    ubs = []\n",
    "    lbs = []\n",
    "    hours = []\n",
    "    for link in links:\n",
    "        category_url = url + link\n",
    "        page = requests.get(category_url)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        postings = soup.findAll(\"div\", class_=\"c-career--dropdown\")\n",
    "        for posting in postings:\n",
    "            ## Also grabbing the department information\n",
    "            dep_name = link.replace('-',' ').replace('/','')\n",
    "            department.append(dep_name)\n",
    "            titles.append(posting.find(\"div\", class_=\"c-career--dropdown__content\").find('h2').text)\n",
    "            details = posting.findAll(\"div\", class_=\"c-job-details__content\")\n",
    "            for detail in details:\n",
    "                detail_title = detail.find('span', class_=\"c-job-details__title\").text\n",
    "                detail_text = detail.find('div', class_=\"c-job-details__text\").text\n",
    "                if detail_title == 'Salary':\n",
    "                    temp = detail_text.replace(',','')\n",
    "                    temp = temp.replace('-','')\n",
    "                    temp = temp.split()\n",
    "                    salary_numbers = [float(s[1:]) for s in temp if s.startswith('£')]\n",
    "                    ## Salary may not be specified\n",
    "                    if len(salary_numbers) > 0:\n",
    "                        ## Salaries are sometimes specified as per week instead of per year\n",
    "                        if 'per' in temp and 'week' in temp:\n",
    "                            salary_numbers = [salary*52 for salary in salary_numbers]\n",
    "                        ubs.append(max(salary_numbers))\n",
    "                        lbs.append(min(salary_numbers))\n",
    "                    else:\n",
    "                        ubs.append(None)\n",
    "                        lbs.append(None)\n",
    "                ## Some postings say \"Benefits\" instead of \"Hours and benefits\", and sometimes the spelling is capitalized differently\n",
    "                elif detail_title.lower() == 'hours and benefits' or detail_title.lower() == 'benefits':\n",
    "                    ## Some postings do not specify a number of hours per week\n",
    "                    work_time = None\n",
    "                    for s in detail_text.split():\n",
    "                        if '-hour' in s:\n",
    "                            work_time = s\n",
    "                            ## Some postings write, e.g., 40-hour per week, some 40-hours per week\n",
    "                            if '-hours' in s:\n",
    "                                work_time = int(work_time.replace('-hours',''))\n",
    "                            else:\n",
    "                                work_time = int(work_time.replace('-hour',''))\n",
    "                    hours.append(work_time)\n",
    "                        \n",
    "    job_data = pd.DataFrame(\n",
    "        {'Department': department,\n",
    "         'Job title': titles,\n",
    "         'Salary lower': lbs,\n",
    "         'Salary upper': ubs,\n",
    "         'Weekly hours': hours\n",
    "        })\n",
    "    return job_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e0bb58",
   "metadata": {},
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a24b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "aldi_job_data = scrape_aldi_jobs()\n",
    "aldi_job_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b86b79",
   "metadata": {},
   "source": [
    "We can now explore the data frame, improve our code if we find issues, and then analyze it. For example, let's have a look at a simple histogram of postings per department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aee99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(30, 10)\n",
    "sns.histplot(data=aldi_job_data, x=\"Department\",ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1fc1d9",
   "metadata": {},
   "source": [
    "Finally, save the job postings we found as a CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce7d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aldi_job_data.to_csv('Aldi_postings_2021-10-20.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e19a87",
   "metadata": {},
   "source": [
    "## 2. A more advanced case - using Selenium to enter details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a406a1",
   "metadata": {},
   "source": [
    "Let's get data from a second competitor. We will use Lidle here (I am, of course, not biased in my choices). Check out Lidl's hiring page https://careers.lidl.co.uk/ and start a search. Then look at the link where you landed at - can you see why things are a bit more complex here?\n",
    "\n",
    "Since we cannot just find the right links, we need to act like a browser. This is where Selenium comes in - it will literally run a browser!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cdaebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff2c8c8",
   "metadata": {},
   "source": [
    "We need to choose the type of browser that Selenium runs - and each comes with its own access and installation requirements. I personally recommend using Chrome. However, to use Chrome, with Selenium, you need to install ChromeDriver. The site https://sites.google.com/chromium.org/driver/downloads gives download files, which should work fine for Windows users. Simply download and unpack the Zip, which gives you a .exe file. Either move it somewhere on your PATH, or add it to your path (https://stackoverflow.com/questions/4822400/register-an-exe-so-you-can-run-it-from-any-command-line-in-windows gives a good description how to).\n",
    "\n",
    "On Mac, you may run into access issues. The easiest way to proceed is to use Homebrew (https://brew.sh/ shows how to use it). Once done, type\n",
    "```\n",
    "brew install chromedriver\n",
    "```\n",
    "into your terminal.\n",
    "Other options can be found here: https://www.kenst.com/2015/03/installing-chromedriver-on-mac-osx/ (note that the syntax can be a bit outdated).\n",
    "\n",
    "Once done, the below code will open a new window in the browser of your choice (here Chrome):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e74df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://careers.lidl.co.uk/jobsearch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0817a32",
   "metadata": {},
   "source": [
    "You will notice that this is a completely new Chrome process - so cookies are not yet accepted. To see what's going on, let's start by accepting cookies. How do we do this? We simply find the right button (by insepcting the site, then copying the XPath), and then let Selenium click this button!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fcafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "cookie_button = driver.find_element_by_xpath('//button[@class=\"cookie-alert-extended-button\"]')\n",
    "cookie_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d597d6b3",
   "metadata": {},
   "source": [
    "The Lidl jobs site offers the option to select head office positions, just like Aldi. However, the link is relatively complex, so we will simply let Selenium click on the right button again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_office_button = driver.find_element_by_xpath('//h4[contains(text(),\"Head Office Roles\")]')\n",
    "head_office_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e12b04d",
   "metadata": {},
   "source": [
    "There are a few positions here. If you click on any of those, you'll notice that the links are relatively simple in structure and don't depend on your website interaction. Hence, the easiest is for us to collect all the posting links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9f39af",
   "metadata": {},
   "outputs": [],
   "source": [
    "posting_urls = []\n",
    "postings = driver.find_elements_by_xpath('//a[@class=\"jobResult\"]')\n",
    "for posting in postings:\n",
    "    posting_urls.append(posting.get_attribute('href'))\n",
    "print(posting_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da6af5",
   "metadata": {},
   "source": [
    "It may be that postings are spread across multiple pages (delete the filters to see this). Luckily, there is a forward button that let's us scroll through the pages. We can easily combine this with our previous code. Note that we only move forward if the next page element actually exists.\n",
    "There can be a problem with identifying the button location. Usually, this can be fixed by maximizing the window in which Selenium runs.\n",
    "\n",
    "\n",
    "Also, we add an implict wait so that the server has time to respond before our clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7818e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = False\n",
    "posting_urls = []\n",
    "while not stop:\n",
    "    driver.implicitly_wait(5)\n",
    "    postings = driver.find_elements_by_xpath('//a[contains(@class,\"jobResult\")]')\n",
    "    print(\"Found \" + str(len(postings)) + \" postings\")\n",
    "    for posting in postings:\n",
    "        posting_urls.append(posting.get_attribute('href'))\n",
    "    next_elements = driver.find_elements_by_class_name('paginationArrow_next')\n",
    "    if len(next_elements) > 0:\n",
    "        element = driver.find_element_by_class_name('paginationArrow_next')\n",
    "        if element.is_enabled():\n",
    "            driver.execute_script(\"arguments[0].click();\", element)\n",
    "            time.sleep(3)\n",
    "        else:\n",
    "            stop = True\n",
    "    else:\n",
    "        stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6d877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidl_job_data = pd.DataFrame({'url': posting_urls})\n",
    "lidl_job_data.to_csv('Lidl_postings_2021-10-20.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67abf1e8",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Now that we have loaded the urls of the relevant vacancies, can you extract key information (e.g., title and postcode of location, maybe also salary)? You might want to take a look at what we did for the Aldi vacancies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dcb2f1",
   "metadata": {},
   "source": [
    "If you are having problems running Selenium, you can use the uploaded list of urls (note: some may no longer be working):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "posting_urls = pd.read_csv('Lidl_postings_2021-10-20.csv')['url'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd2943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtvc_env",
   "language": "python",
   "name": "dtvc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
